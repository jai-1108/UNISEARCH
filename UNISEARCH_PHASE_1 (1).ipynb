{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24ff69ee190e4d748f4d566e9a2306a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42d9381222a54150bc5d6ca11e4c3264",
              "IPY_MODEL_218385697efe4de7b91e08b881fba55b",
              "IPY_MODEL_9b1180f58230494fb1c5551448a83c56"
            ],
            "layout": "IPY_MODEL_8619d9bbed104cb1bd95d4dc94a45892"
          }
        },
        "42d9381222a54150bc5d6ca11e4c3264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_602a16ae013a4ac39c1355dc0125c1c8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8fe7d4e5ceb348f884bfebd19bd2f578",
            "value": "Batches:‚Äá100%"
          }
        },
        "218385697efe4de7b91e08b881fba55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ee62543617c4096a1b93c0131e6cd2e",
            "max": 1192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d634e49d53f84c30b1bb7f0226e95562",
            "value": 1192
          }
        },
        "9b1180f58230494fb1c5551448a83c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2da38cbf537b44d69a02e06de74c569d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b5cbe87c968741b1ab55e3efd892c6db",
            "value": "‚Äá1192/1192‚Äá[12:53&lt;00:00,‚Äá‚Äá5.78it/s]"
          }
        },
        "8619d9bbed104cb1bd95d4dc94a45892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "602a16ae013a4ac39c1355dc0125c1c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fe7d4e5ceb348f884bfebd19bd2f578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ee62543617c4096a1b93c0131e6cd2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d634e49d53f84c30b1bb7f0226e95562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2da38cbf537b44d69a02e06de74c569d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5cbe87c968741b1ab55e3efd892c6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pnuKJUXyNhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "411042b5-374c-4be3-b412-8dccb2312ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ Installed external packages.\n",
            "‚úÖ Core Python modules imported.\n",
            "Python version: 2.8.0+cu126 (PyTorch version printed here)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Cell 1: Install core libraries + basic imports\n",
        "# ------------------------------------------------------------\n",
        "# This cell:\n",
        "#   1. Installs all the external Python packages we need.\n",
        "#   2. Imports commonly used standard libraries.\n",
        "#   3. Does NOT touch your data or Google Drive yet.\n",
        "#\n",
        "# You should run this first in a fresh Colab runtime.\n",
        "# ============================================================\n",
        "\n",
        "# ---- 1. Install external packages (quietly) -----------------\n",
        "# faiss-cpu           -> vector search / ANN index\n",
        "# sentence-transformers -> text embeddings for lectures + papers\n",
        "# clip-anytorch       -> CLIP model for image <-> text alignment\n",
        "# openai-whisper      -> automatic speech recognition for lectures\n",
        "# PyMuPDF             -> robust PDF text extraction\n",
        "# tqdm                -> progress bars for long loops\n",
        "\n",
        "!pip -q install faiss-cpu sentence-transformers clip-anytorch \\\n",
        "                 openai-whisper PyMuPDF==1.24.10 tqdm\n",
        "\n",
        "print(\"‚úÖ Installed external packages.\")\n",
        "\n",
        "# ---- 2. Standard library imports ----------------------------\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# ---- 3. Numeric + ML utilities ------------------------------\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# We won't load any heavy models yet. That comes later in the\n",
        "# embeddings & transcription stages. For now, just confirm setup.\n",
        "print(\"‚úÖ Core Python modules imported.\")\n",
        "print(\"Python version:\", torch.__version__, \"(PyTorch version printed here)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 2: Mount Google Drive + create UNISEARCH_MASTER project\n",
        "# ------------------------------------------------------------\n",
        "# This cell:\n",
        "#   1. Mounts your Google Drive.\n",
        "#   2. Creates a NEW isolated project directory:\n",
        "#           /MyDrive/UNISEARCH_MASTER\n",
        "#      so nothing interferes with previous attempts.\n",
        "#   3. Sets up the full folder structure for the entire pipeline.\n",
        "#   4. Scans raw/videos and raw/papers to show what data is present.\n",
        "#   5. Writes a config.json \"master reference\" for Colab 1 & 2.\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# ---- 1. Mount Google Drive ---------------------------------\n",
        "print(\"üîå Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---- 2. Define the NEW unique project root -----------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "\n",
        "# ---- 3. Define subdirectories ------------------------------\n",
        "RAW_DIR         = BASE_DIR / \"raw\"\n",
        "RAW_VIDEOS      = RAW_DIR / \"videos\"\n",
        "RAW_PAPERS      = RAW_DIR / \"papers\"\n",
        "\n",
        "PROC_DIR        = BASE_DIR / \"processed\"\n",
        "KEYFRAMES_DIR   = PROC_DIR / \"keyframes\"\n",
        "TRANSCRIPTS_DIR = PROC_DIR / \"transcripts\"\n",
        "MANIFESTS_DIR   = PROC_DIR / \"manifests\"\n",
        "EMB_DIR         = PROC_DIR / \"embeddings\"\n",
        "INDICES_DIR     = PROC_DIR / \"indices\"\n",
        "\n",
        "# Create all directories\n",
        "for d in [\n",
        "    RAW_VIDEOS, RAW_PAPERS,\n",
        "    KEYFRAMES_DIR, TRANSCRIPTS_DIR,\n",
        "    MANIFESTS_DIR, EMB_DIR, INDICES_DIR\n",
        "]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ Project root directory:\", BASE_DIR)\n",
        "print(\"üìÅ Raw videos directory   :\", RAW_VIDEOS)\n",
        "print(\"üìÅ Raw papers directory   :\", RAW_PAPERS)\n",
        "print(\"üìÅ Processed directory    :\", PROC_DIR)\n",
        "\n",
        "# ---- 4. Inventory of raw data ------------------------------\n",
        "VIDEO_EXTS = [\".mp4\", \".mkv\", \".avi\", \".mov\", \".webm\", \".m4v\"]\n",
        "\n",
        "video_files = [\n",
        "    p for p in RAW_VIDEOS.iterdir()\n",
        "    if p.is_file() and p.suffix.lower() in VIDEO_EXTS\n",
        "]\n",
        "\n",
        "paper_files = list(RAW_PAPERS.glob(\"*.pdf\"))\n",
        "\n",
        "print(f\"\\nüé• Found {len(video_files)} video files in raw/videos\")\n",
        "for v in video_files[:10]:\n",
        "    print(\"   -\", v.name)\n",
        "if len(video_files) > 10:\n",
        "    print(f\"   ... and {len(video_files) - 10} more\")\n",
        "\n",
        "print(f\"\\nüìÑ Found {len(paper_files)} PDF files in raw/papers\")\n",
        "for p in paper_files[:10]:\n",
        "    print(\"   -\", p.name)\n",
        "if len(paper_files) > 10:\n",
        "    print(f\"   ... and {len(paper_files) - 10} more\")\n",
        "\n",
        "# ---- 5. Write a fresh config.json ---------------------------\n",
        "config_path = BASE_DIR / \"config.json\"\n",
        "\n",
        "config = {\n",
        "    \"paths\": {\n",
        "        \"base_dir\": str(BASE_DIR),\n",
        "        \"raw_videos\": str(RAW_VIDEOS),\n",
        "        \"raw_papers\": str(RAW_PAPERS),\n",
        "        \"processed\": str(PROC_DIR),\n",
        "        \"keyframes\": str(KEYFRAMES_DIR),\n",
        "        \"transcripts\": str(TRANSCRIPTS_DIR),\n",
        "        \"manifests\": str(MANIFESTS_DIR),\n",
        "        \"embeddings\": str(EMB_DIR),\n",
        "        \"indices\": str(INDICES_DIR),\n",
        "    },\n",
        "    \"models\": {\n",
        "        # Primary semantic text model (for lecture + paper text retrieval)\n",
        "        \"text_semantic_model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "        # Cross-modal CLIP model\n",
        "        \"clip_model\": \"openai/clip-vit-base-patch32\"\n",
        "    },\n",
        "    \"embedding_dims\": {\n",
        "        # We will fill these in AFTER loading models (later cells)\n",
        "        \"text_semantic_dim\": None,\n",
        "        \"clip_dim\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ config.json written to:\", config_path)\n"
      ],
      "metadata": {
        "id": "sRMnCqnhy1UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ca7e11-7c6f-4693-84f5-a6cf20c68b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîå Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "üìÅ Project root directory: /content/drive/MyDrive/UNISEARCH_MASTER\n",
            "üìÅ Raw videos directory   : /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos\n",
            "üìÅ Raw papers directory   : /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers\n",
            "üìÅ Processed directory    : /content/drive/MyDrive/UNISEARCH_MASTER/processed\n",
            "\n",
            "üé• Found 0 video files in raw/videos\n",
            "\n",
            "üìÑ Found 42 PDF files in raw/papers\n",
            "   - alexnet_2012_imagenet.pdf\n",
            "   - vgg_2014_very_deep.pdf\n",
            "   - googlenet_2014_inception.pdf\n",
            "   - resnet_2015_deep_residual.pdf\n",
            "   - densenet_2016.pdf\n",
            "   - fcn_2014_fully_conv_networks.pdf\n",
            "   - mask_rcnn_2017.pdf\n",
            "   - yolov1_2015.pdf\n",
            "   - yolov3_2018.pdf\n",
            "   - mobilenet_v1_2017.pdf\n",
            "   ... and 32 more\n",
            "\n",
            "‚úÖ config.json written to: /content/drive/MyDrive/UNISEARCH_MASTER/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CELL 3 ‚Äî FULL DATASET LISTING\n",
        "# ----------------------------------------------\n",
        "# This cell:\n",
        "#   - Loads config.json\n",
        "#   - Lists ALL lecture videos (every filename) in non-empty course folders\n",
        "#   - Lists ALL research paper PDFs (every filename)\n",
        "# ==============================================\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "VIDEOS_ROOT = PROJECT_ROOT / \"raw/videos\"\n",
        "PAPERS_ROOT = PROJECT_ROOT / \"raw/papers\"\n",
        "CONFIG_PATH = PROJECT_ROOT / \"config.json\"\n",
        "\n",
        "print(\"üìÅ PROJECT ROOT:\", PROJECT_ROOT)\n",
        "print(\"üìÅ VIDEOS ROOT :\", VIDEOS_ROOT)\n",
        "print(\"üìÅ PAPERS ROOT :\", PAPERS_ROOT)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load config.json\n",
        "# -------------------------------\n",
        "print(\"\\nüìÑ Checking config.json...\")\n",
        "if CONFIG_PATH.exists():\n",
        "    with open(CONFIG_PATH, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    print(\"   ‚úì config.json loaded successfully\")\n",
        "else:\n",
        "    print(\"   ‚ùå config.json NOT found ‚Äî go back to Cell 2\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. List ALL videos (by course)\n",
        "# -------------------------------\n",
        "print(\"\\nüé• Listing ALL lecture videos (non-empty course folders only)...\")\n",
        "\n",
        "if not VIDEOS_ROOT.exists():\n",
        "    print(\"   ‚ùå raw/videos folder missing!\")\n",
        "else:\n",
        "    course_folders = [d for d in VIDEOS_ROOT.iterdir() if d.is_dir()]\n",
        "    valid_courses = []\n",
        "\n",
        "    for course in sorted(course_folders):\n",
        "        vids = sorted([v for v in course.iterdir() if v.is_file()])\n",
        "        if len(vids) > 0:\n",
        "            valid_courses.append((course, vids))\n",
        "\n",
        "    if not valid_courses:\n",
        "        print(\"   ‚ùå No NON-EMPTY course folders found under raw/videos\")\n",
        "    else:\n",
        "        print(f\"   ‚úì Found {len(valid_courses)} non-empty course folders.\\n\")\n",
        "        for course, vids in valid_courses:\n",
        "            print(f\"==============================\")\n",
        "            print(f\"‚ñ∫ {course.name} ‚Äî {len(vids)} videos\")\n",
        "            print(f\"==============================\")\n",
        "            for v in vids:\n",
        "                print(\"   ‚Ä¢\", v.name)\n",
        "            print()  # blank line between courses\n",
        "\n",
        "# -------------------------------\n",
        "# 3. List ALL research papers\n",
        "# -------------------------------\n",
        "print(\"\\nüìö Listing ALL research papers (PDFs)...\")\n",
        "\n",
        "if not PAPERS_ROOT.exists():\n",
        "    print(\"   ‚ùå raw/papers folder missing!\")\n",
        "else:\n",
        "    papers = sorted([p for p in PAPERS_ROOT.iterdir() if p.suffix.lower() == \".pdf\"])\n",
        "    print(f\"   ‚úì Found {len(papers)} papers:\\n\")\n",
        "    for p in papers:\n",
        "        print(\"   ‚Ä¢\", p.name)\n",
        "\n",
        "print(\"\\n‚úÖ FULL DATASET LISTING COMPLETE.\")\n"
      ],
      "metadata": {
        "id": "zI90kdgUBvdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab87275-49f6-48ba-a600-c133b9ea32b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ PROJECT ROOT: /content/drive/MyDrive/UNISEARCH_MASTER\n",
            "üìÅ VIDEOS ROOT : /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos\n",
            "üìÅ PAPERS ROOT : /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers\n",
            "\n",
            "üìÑ Checking config.json...\n",
            "   ‚úì config.json loaded successfully\n",
            "\n",
            "üé• Listing ALL lecture videos (non-empty course folders only)...\n",
            "   ‚úì Found 2 non-empty course folders.\n",
            "\n",
            "==============================\n",
            "‚ñ∫ CS229 ‚Äî 20 videos\n",
            "==============================\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_01_jGwO_UgTS7I.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_02_4b4MUYve_U8.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_03_het9HFqo1TQ.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_04_iZTeva0WSTQ.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_05_nt63k3bfXS0.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_06_lDwow4aOrtg.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_07_8NYoQiRANpg.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_08_rjbkWSTjHzM.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_09_iVOxMcumR4A.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_10_wr9gUr-eWdA.webm\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_11_MfIjxPh6Pys.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_12_zUazLXZZA2U.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_13_ORrStCArmP4.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_14_rVfZHWTwXSA.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_15_tw6cmL5STuY.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_16_YQA9lLdLig8.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_17_d5gaWTo6kDM.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_18_QFu5nuc-S0s.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_19_0rt2CsEQv6U.mkv\n",
            "   ‚Ä¢ Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_20_pLhPQynL0tY.mkv\n",
            "\n",
            "==============================\n",
            "‚ñ∫ MIT_6_034 ‚Äî 24 videos\n",
            "==============================\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_01_TjZBTDzGeGg.webm\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_03_leXa7EKUPFk.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_04_j1H3jAAGlEA.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_05_gGQ-vAmdAOI.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_06_STjW3eH0Cik.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_07_l-tzjenXrvI.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_08_dARl_gGrS4o.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_09_gvmfbePC2pc.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_10_09mb78oiPkA.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_11_SXBG3RGr_Rc.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_12_uXt8qF2Zzfo.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_13_VrMHA3yX_QI.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_14_kHyNqSnzP8Y.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_15_L73hY1pBcQI.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_16_sh3EPjhhd40.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_17__PwhiWxHK8o.webm\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_18_UHBmv7qCey4.mp4\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_19_bQI0OmJPby4.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_20_PimSbFGrwXM.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_21_A6Ud6oUCRak.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_22_EC6bf8JCpDQ.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_23_XPEJg_6Cg6o.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_24_iusTmgQyZ44.mkv\n",
            "   ‚Ä¢ MIT 6.034 Artificial Intelligence, Fall 2010_25_Tl_p5pgBsyM.mp4\n",
            "\n",
            "\n",
            "üìö Listing ALL research papers (PDFs)...\n",
            "   ‚úì Found 42 papers:\n",
            "\n",
            "   ‚Ä¢ adam_2014.pdf\n",
            "   ‚Ä¢ albef_2021.pdf\n",
            "   ‚Ä¢ albert_2019.pdf\n",
            "   ‚Ä¢ alexnet_2012_imagenet.pdf\n",
            "   ‚Ä¢ align_2021.pdf\n",
            "   ‚Ä¢ attention_is_all_you_need_2017.pdf\n",
            "   ‚Ä¢ bahdanau_attention_2014.pdf\n",
            "   ‚Ä¢ batchnorm_2015.pdf\n",
            "   ‚Ä¢ bert_2018.pdf\n",
            "   ‚Ä¢ blip2_2023.pdf\n",
            "   ‚Ä¢ blip_2022.pdf\n",
            "   ‚Ä¢ byol_2020.pdf\n",
            "   ‚Ä¢ clip_2021.pdf\n",
            "   ‚Ä¢ cpc_2018.pdf\n",
            "   ‚Ä¢ densenet_2016.pdf\n",
            "   ‚Ä¢ distilbert_2019.pdf\n",
            "   ‚Ä¢ efficientnet_2019.pdf\n",
            "   ‚Ä¢ elmo_2018_deep_contextualized.pdf\n",
            "   ‚Ä¢ fcn_2014_fully_conv_networks.pdf\n",
            "   ‚Ä¢ glove_2014.pdf\n",
            "   ‚Ä¢ googlenet_2014_inception.pdf\n",
            "   ‚Ä¢ gpt2_2019.pdf\n",
            "   ‚Ä¢ gpt3_2020.pdf\n",
            "   ‚Ä¢ llava_2023.pdf\n",
            "   ‚Ä¢ longformer_2020.pdf\n",
            "   ‚Ä¢ mask_rcnn_2017.pdf\n",
            "   ‚Ä¢ mobilenet_v1_2017.pdf\n",
            "   ‚Ä¢ mobilenet_v2_2018.pdf\n",
            "   ‚Ä¢ moco_v1_2019.pdf\n",
            "   ‚Ä¢ moco_v2_2020.pdf\n",
            "   ‚Ä¢ resnet_2015_deep_residual.pdf\n",
            "   ‚Ä¢ roberta_2019.pdf\n",
            "   ‚Ä¢ seq2seq_2014.pdf\n",
            "   ‚Ä¢ simclr_2020.pdf\n",
            "   ‚Ä¢ t5_2019_exploring_tl.pdf\n",
            "   ‚Ä¢ unet_2015.pdf\n",
            "   ‚Ä¢ vgg_2014_very_deep.pdf\n",
            "   ‚Ä¢ vilbert_2019.pdf\n",
            "   ‚Ä¢ word2vec_2013.pdf\n",
            "   ‚Ä¢ xlnet_2019.pdf\n",
            "   ‚Ä¢ yolov1_2015.pdf\n",
            "   ‚Ä¢ yolov3_2018.pdf\n",
            "\n",
            "‚úÖ FULL DATASET LISTING COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CELL 4 ‚Äî Build Manifests for Lectures & Papers\n",
        "# ----------------------------------------------\n",
        "# This cell scans:\n",
        "#   - raw/videos/<course>/<video_file>\n",
        "#   - raw/papers/*.pdf\n",
        "#\n",
        "# And builds:\n",
        "#   - processed/manifests/video_manifest.jsonl\n",
        "#   - processed/manifests/paper_manifest.jsonl\n",
        "#\n",
        "# Each line is a JSON object (JSONL format) with a stable ID and\n",
        "# a relative file path (from the project root).\n",
        "# ==============================================\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "RAW_VIDEOS = PROJECT_ROOT / \"raw\" / \"videos\"\n",
        "RAW_PAPERS = PROJECT_ROOT / \"raw\" / \"papers\"\n",
        "MANIFEST_DIR = PROJECT_ROOT / \"processed\" / \"manifests\"\n",
        "MANIFEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "VIDEO_MANIFEST_PATH = MANIFEST_DIR / \"video_manifest.jsonl\"\n",
        "PAPER_MANIFEST_PATH = MANIFEST_DIR / \"paper_manifest.jsonl\"\n",
        "\n",
        "def slugify(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Turn arbitrary text (like a filename stem) into a safe ID:\n",
        "    - lowercase\n",
        "    - spaces -> _\n",
        "    - keep letters, numbers, _, -\n",
        "    - drop everything else (colons, weird unicode, etc.)\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.replace(\" \", \"_\")\n",
        "    text = re.sub(r\"[^a-z0-9_\\-]+\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Build video_manifest.jsonl\n",
        "# -------------------------------\n",
        "video_records = []\n",
        "video_exts = {\".mp4\", \".mkv\", \".avi\", \".mov\", \".webm\", \".m4v\"}\n",
        "\n",
        "if not RAW_VIDEOS.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå raw/videos folder not found at {RAW_VIDEOS}\")\n",
        "\n",
        "for course_dir in sorted(RAW_VIDEOS.iterdir()):\n",
        "    if not course_dir.is_dir():\n",
        "        continue\n",
        "    course_name = course_dir.name  # e.g. \"MIT_6_034\" or \"CS229\"\n",
        "\n",
        "    videos = sorted(\n",
        "        [v for v in course_dir.iterdir() if v.is_file() and v.suffix.lower() in video_exts]\n",
        "    )\n",
        "    for idx, v in enumerate(videos, start=1):\n",
        "        # Create a stable ID like: mit_6_034__01_tjzbt...\n",
        "        base_stem = slugify(v.stem)\n",
        "        video_id = f\"{slugify(course_name)}__{idx:02d}_{base_stem[:40]}\"\n",
        "\n",
        "        # Store path relative to project root (portable)\n",
        "        rel_path = v.relative_to(PROJECT_ROOT)\n",
        "\n",
        "        record = {\n",
        "            \"video_id\": video_id,\n",
        "            \"course\": course_name,\n",
        "            \"file_path\": str(rel_path),\n",
        "            \"file_name\": v.name,\n",
        "            \"index_in_course\": idx,\n",
        "        }\n",
        "        video_records.append(record)\n",
        "\n",
        "with VIDEO_MANIFEST_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for rec in video_records:\n",
        "        f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "print(f\"üé• Wrote video manifest: {VIDEO_MANIFEST_PATH}\")\n",
        "print(f\"   Total videos indexed: {len(video_records)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Build paper_manifest.jsonl\n",
        "# -------------------------------\n",
        "paper_records = []\n",
        "\n",
        "if not RAW_PAPERS.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå raw/papers folder not found at {RAW_PAPERS}\")\n",
        "\n",
        "papers = sorted([p for p in RAW_PAPERS.iterdir() if p.suffix.lower() == \".pdf\"])\n",
        "\n",
        "for idx, p in enumerate(papers, start=1):\n",
        "    # Example: paper_id = \"resnet_2015_deep_residual\"\n",
        "    base_stem = slugify(p.stem)\n",
        "    paper_id = f\"paper_{idx:03d}_{base_stem[:40]}\"\n",
        "\n",
        "    rel_path = p.relative_to(PROJECT_ROOT)\n",
        "\n",
        "    # A simple \"title\" derived from filename (underscores -> spaces)\n",
        "    naive_title = p.stem.replace(\"_\", \" \")\n",
        "\n",
        "    record = {\n",
        "        \"paper_id\": paper_id,\n",
        "        \"file_path\": str(rel_path),\n",
        "        \"file_name\": p.name,\n",
        "        \"title_guess\": naive_title,\n",
        "        \"index\": idx,\n",
        "    }\n",
        "    paper_records.append(record)\n",
        "\n",
        "with PAPER_MANIFEST_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for rec in paper_records:\n",
        "        f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "print(f\"üìö Wrote paper manifest: {PAPER_MANIFEST_PATH}\")\n",
        "print(f\"   Total papers indexed: {len(paper_records)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Quick peek at a few records\n",
        "# -------------------------------\n",
        "print(\"\\nüîé Sample video records:\")\n",
        "for rec in video_records[:3]:\n",
        "    print(\"  \", rec)\n",
        "\n",
        "print(\"\\nüîé Sample paper records:\")\n",
        "for rec in paper_records[:3]:\n",
        "    print(\"  \", rec)\n",
        "\n",
        "print(\"\\n‚úÖ CELL 4 complete (manifests built).\")\n"
      ],
      "metadata": {
        "id": "aK1DClAFJah8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e776fbe-1d5e-40c1-f8cb-f777542f1fc8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé• Wrote video manifest: /content/drive/MyDrive/UNISEARCH_MASTER/processed/manifests/video_manifest.jsonl\n",
            "   Total videos indexed: 44\n",
            "üìö Wrote paper manifest: /content/drive/MyDrive/UNISEARCH_MASTER/processed/manifests/paper_manifest.jsonl\n",
            "   Total papers indexed: 42\n",
            "\n",
            "üîé Sample video records:\n",
            "   {'video_id': 'cs229__01_stanford_cs229_machine_learning_full_cou', 'course': 'CS229', 'file_path': 'raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_01_jGwO_UgTS7I.webm', 'file_name': 'Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_01_jGwO_UgTS7I.webm', 'index_in_course': 1}\n",
            "   {'video_id': 'cs229__02_stanford_cs229_machine_learning_full_cou', 'course': 'CS229', 'file_path': 'raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_02_4b4MUYve_U8.webm', 'file_name': 'Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_02_4b4MUYve_U8.webm', 'index_in_course': 2}\n",
            "   {'video_id': 'cs229__03_stanford_cs229_machine_learning_full_cou', 'course': 'CS229', 'file_path': 'raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_03_het9HFqo1TQ.webm', 'file_name': 'Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_03_het9HFqo1TQ.webm', 'index_in_course': 3}\n",
            "\n",
            "üîé Sample paper records:\n",
            "   {'paper_id': 'paper_001_adam_2014', 'file_path': 'raw/papers/adam_2014.pdf', 'file_name': 'adam_2014.pdf', 'title_guess': 'adam 2014', 'index': 1}\n",
            "   {'paper_id': 'paper_002_albef_2021', 'file_path': 'raw/papers/albef_2021.pdf', 'file_name': 'albef_2021.pdf', 'title_guess': 'albef 2021', 'index': 2}\n",
            "   {'paper_id': 'paper_003_albert_2019', 'file_path': 'raw/papers/albert_2019.pdf', 'file_name': 'albert_2019.pdf', 'title_guess': 'albert 2019', 'index': 3}\n",
            "\n",
            "‚úÖ CELL 4 complete (manifests built).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CELL 5 ‚Äî Transcribe Lectures with Whisper\n",
        "# ----------------------------------------------\n",
        "# Uses:\n",
        "#   - processed/manifests/video_manifest.jsonl\n",
        "#\n",
        "# Produces:\n",
        "#   - processed/transcripts/{video_id}.json\n",
        "#\n",
        "# Each transcript file is a JSON with:\n",
        "#   {\n",
        "#     \"video_id\": ...,\n",
        "#     \"course\": ...,\n",
        "#     \"file_path\": ...,\n",
        "#     \"whisper_model\": ...,\n",
        "#     \"segments\": [\n",
        "#        {\"id\": 0, \"start\": ..., \"end\": ..., \"text\": \"...\"},\n",
        "#        ...\n",
        "#     ]\n",
        "#   }\n",
        "#\n",
        "# Safe to re-run:\n",
        "#   - Skips videos that already have a transcript JSON file.\n",
        "# ==============================================\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import whisper\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "VIDEO_MANIFEST_PATH = PROJECT_ROOT / \"processed\" / \"manifests\" / \"video_manifest.jsonl\"\n",
        "TRANSCRIPTS_DIR = PROJECT_ROOT / \"processed\" / \"transcripts\"\n",
        "TRANSCRIPTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "AUDIO_CACHE = Path(\"/content/audio_cache\")\n",
        "AUDIO_CACHE.mkdir(exist_ok=True)\n",
        "\n",
        "# Choose Whisper model:\n",
        "#   \"base\"  ‚Üí faster, lower quality\n",
        "#   \"small\" ‚Üí balance\n",
        "#   \"medium\"/\"large\" ‚Üí slower, best quality (expensive)\n",
        "WHISPER_MODEL_NAME = \"small\"\n",
        "\n",
        "print(f\"üéß Loading Whisper model: {WHISPER_MODEL_NAME} ...\")\n",
        "whisper_model = whisper.load_model(WHISPER_MODEL_NAME)\n",
        "print(\"   ‚úì Model loaded\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load video manifest\n",
        "# -------------------------------\n",
        "if not VIDEO_MANIFEST_PATH.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå video_manifest.jsonl not found at {VIDEO_MANIFEST_PATH}\")\n",
        "\n",
        "video_records = []\n",
        "with VIDEO_MANIFEST_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        video_records.append(json.loads(line))\n",
        "\n",
        "print(f\"\\nüìù Loaded {len(video_records)} video records from manifest.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Helper: extract audio with ffmpeg\n",
        "# -------------------------------\n",
        "def extract_audio(input_video: Path, output_audio: Path):\n",
        "    \"\"\"\n",
        "    Extracts audio track from the video using ffmpeg.\n",
        "    Output is a mono 16kHz wav, which Whisper likes.\n",
        "    \"\"\"\n",
        "    if output_audio.exists():\n",
        "        return\n",
        "\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\n",
        "        \"-y\",  # overwrite\n",
        "        \"-i\", str(input_video),\n",
        "        \"-ac\", \"1\",          # mono\n",
        "        \"-ar\", \"16000\",      # 16kHz\n",
        "        \"-vn\",               # no video\n",
        "        \"-f\", \"wav\",\n",
        "        str(output_audio),\n",
        "    ]\n",
        "    print(f\"   üéôÔ∏è  Extracting audio: {input_video.name} -> {output_audio.name}\")\n",
        "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(\"   ‚ùå ffmpeg failed\")\n",
        "        print(\"   STDERR (truncated):\", result.stderr[:400])\n",
        "        raise RuntimeError(f\"ffmpeg failed for {input_video}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Main loop: transcribe each video\n",
        "# -------------------------------\n",
        "for i, rec in enumerate(video_records, start=1):\n",
        "    video_id = rec[\"video_id\"]\n",
        "    rel_path = rec[\"file_path\"]\n",
        "    video_path = PROJECT_ROOT / rel_path\n",
        "\n",
        "    out_json = TRANSCRIPTS_DIR / f\"{video_id}.json\"\n",
        "\n",
        "    print(f\"\\n[{i}/{len(video_records)}] üé¨ {video_id}\")\n",
        "    print(f\"   Video file:\", video_path)\n",
        "\n",
        "    if out_json.exists():\n",
        "        print(\"   ‚è© Transcript already exists, skipping.\")\n",
        "        continue\n",
        "\n",
        "    if not video_path.exists():\n",
        "        print(\"   ‚ùå Video file missing, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 3.1 Extract audio to /content/audio_cache\n",
        "    audio_path = AUDIO_CACHE / f\"{video_id}.wav\"\n",
        "    extract_audio(video_path, audio_path)\n",
        "\n",
        "    # 3.2 Run Whisper transcription\n",
        "    print(\"   üß† Running Whisper transcription...\")\n",
        "    result = whisper_model.transcribe(str(audio_path), language=\"en\", verbose=False)\n",
        "\n",
        "    # 3.3 Build a compact transcript structure\n",
        "    segments = []\n",
        "    for seg in result.get(\"segments\", []):\n",
        "        segments.append(\n",
        "            {\n",
        "                \"id\": seg.get(\"id\"),\n",
        "                \"start\": seg.get(\"start\"),\n",
        "                \"end\": seg.get(\"end\"),\n",
        "                \"text\": seg.get(\"text\"),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    transcript_record = {\n",
        "        \"video_id\": video_id,\n",
        "        \"course\": rec.get(\"course\"),\n",
        "        \"file_path\": rel_path,\n",
        "        \"whisper_model\": WHISPER_MODEL_NAME,\n",
        "        \"segments\": segments,\n",
        "    }\n",
        "\n",
        "    with out_json.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(transcript_record, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"   ‚úì Saved transcript:\", out_json.name)\n",
        "    print(f\"   ‚Ü™ Segments: {len(segments)}\")\n",
        "\n",
        "print(\"\\n‚úÖ CELL 5 complete ‚Äî transcripts generated (or skipped if existing).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY1K8ftkLw53",
        "outputId": "8857d791-6e1f-46d6-895f-1de545c0a2e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéß Loading Whisper model: small ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461M/461M [00:01<00:00, 315MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Model loaded\n",
            "\n",
            "üìù Loaded 44 video records from manifest.\n",
            "\n",
            "[1/44] üé¨ cs229__01_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_01_jGwO_UgTS7I.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[2/44] üé¨ cs229__02_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_02_4b4MUYve_U8.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[3/44] üé¨ cs229__03_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_03_het9HFqo1TQ.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[4/44] üé¨ cs229__04_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_04_iZTeva0WSTQ.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[5/44] üé¨ cs229__05_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_05_nt63k3bfXS0.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[6/44] üé¨ cs229__06_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_06_lDwow4aOrtg.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[7/44] üé¨ cs229__07_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_07_8NYoQiRANpg.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[8/44] üé¨ cs229__08_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_08_rjbkWSTjHzM.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[9/44] üé¨ cs229__09_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_09_iVOxMcumR4A.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[10/44] üé¨ cs229__10_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_10_wr9gUr-eWdA.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[11/44] üé¨ cs229__11_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_11_MfIjxPh6Pys.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[12/44] üé¨ cs229__12_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_12_zUazLXZZA2U.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[13/44] üé¨ cs229__13_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_13_ORrStCArmP4.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[14/44] üé¨ cs229__14_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_14_rVfZHWTwXSA.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[15/44] üé¨ cs229__15_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_15_tw6cmL5STuY.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[16/44] üé¨ cs229__16_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_16_YQA9lLdLig8.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[17/44] üé¨ cs229__17_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_17_d5gaWTo6kDM.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[18/44] üé¨ cs229__18_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_18_QFu5nuc-S0s.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[19/44] üé¨ cs229__19_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_19_0rt2CsEQv6U.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[20/44] üé¨ cs229__20_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_20_pLhPQynL0tY.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[21/44] üé¨ mit_6_034__01_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_01_TjZBTDzGeGg.webm\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[22/44] üé¨ mit_6_034__02_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_03_leXa7EKUPFk.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[23/44] üé¨ mit_6_034__03_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_04_j1H3jAAGlEA.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[24/44] üé¨ mit_6_034__04_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_05_gGQ-vAmdAOI.mkv\n",
            "   ‚è© Transcript already exists, skipping.\n",
            "\n",
            "[25/44] üé¨ mit_6_034__05_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_06_STjW3eH0Cik.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_06_STjW3eH0Cik.mkv -> mit_6_034__05_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 289619/289619 [02:43<00:00, 1776.26frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__05_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 867\n",
            "\n",
            "[26/44] üé¨ mit_6_034__06_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_07_l-tzjenXrvI.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_07_l-tzjenXrvI.mkv -> mit_6_034__06_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 295210/295210 [02:35<00:00, 1893.22frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__06_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 631\n",
            "\n",
            "[27/44] üé¨ mit_6_034__07_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_08_dARl_gGrS4o.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_08_dARl_gGrS4o.mkv -> mit_6_034__07_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 270589/270589 [02:14<00:00, 2016.36frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__07_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 616\n",
            "\n",
            "[28/44] üé¨ mit_6_034__08_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_09_gvmfbePC2pc.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_09_gvmfbePC2pc.mkv -> mit_6_034__08_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 308548/309191 [02:44<00:00, 1881.29frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__08_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 605\n",
            "\n",
            "[29/44] üé¨ mit_6_034__09_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_10_09mb78oiPkA.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_10_09mb78oiPkA.mkv -> mit_6_034__09_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298022/299531 [02:54<00:00, 1711.53frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__09_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 678\n",
            "\n",
            "[30/44] üé¨ mit_6_034__10_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_11_SXBG3RGr_Rc.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_11_SXBG3RGr_Rc.mkv -> mit_6_034__10_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 297667/297667 [02:39<00:00, 1868.80frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__10_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 596\n",
            "\n",
            "[31/44] üé¨ mit_6_034__11_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_12_uXt8qF2Zzfo.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_12_uXt8qF2Zzfo.mkv -> mit_6_034__11_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 304246/304246 [02:45<00:00, 1841.20frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__11_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 883\n",
            "\n",
            "[32/44] üé¨ mit_6_034__12_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_13_VrMHA3yX_QI.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_13_VrMHA3yX_QI.mkv -> mit_6_034__12_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 294528/294528 [02:34<00:00, 1904.67frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__12_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 814\n",
            "\n",
            "[33/44] üé¨ mit_6_034__13_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_14_kHyNqSnzP8Y.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_14_kHyNqSnzP8Y.mkv -> mit_6_034__13_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 283595/283595 [02:20<00:00, 2012.74frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__13_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 579\n",
            "\n",
            "[34/44] üé¨ mit_6_034__14_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_15_L73hY1pBcQI.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_15_L73hY1pBcQI.mkv -> mit_6_034__14_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 286888/286888 [02:19<00:00, 2063.06frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__14_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 494\n",
            "\n",
            "[35/44] üé¨ mit_6_034__15_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_16_sh3EPjhhd40.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_16_sh3EPjhhd40.mkv -> mit_6_034__15_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 281348/281348 [02:42<00:00, 1732.72frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__15_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 737\n",
            "\n",
            "[36/44] üé¨ mit_6_034__16_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_17__PwhiWxHK8o.webm\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_17__PwhiWxHK8o.webm -> mit_6_034__16_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 297386/297386 [02:30<00:00, 1975.19frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__16_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 598\n",
            "\n",
            "[37/44] üé¨ mit_6_034__17_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_18_UHBmv7qCey4.mp4\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_18_UHBmv7qCey4.mp4 -> mit_6_034__17_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 309772/310000 [02:48<00:00, 1837.56frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__17_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 578\n",
            "\n",
            "[38/44] üé¨ mit_6_034__18_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_19_bQI0OmJPby4.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_19_bQI0OmJPby4.mkv -> mit_6_034__18_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293756/293756 [02:22<00:00, 2067.11frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__18_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 559\n",
            "\n",
            "[39/44] üé¨ mit_6_034__19_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_20_PimSbFGrwXM.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_20_PimSbFGrwXM.mkv -> mit_6_034__19_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 294561/294561 [02:21<00:00, 2076.62frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__19_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 518\n",
            "\n",
            "[40/44] üé¨ mit_6_034__20_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_21_A6Ud6oUCRak.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_21_A6Ud6oUCRak.mkv -> mit_6_034__20_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290910/290910 [02:24<00:00, 2013.54frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__20_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 456\n",
            "\n",
            "[41/44] üé¨ mit_6_034__21_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_22_EC6bf8JCpDQ.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_22_EC6bf8JCpDQ.mkv -> mit_6_034__21_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 292543/292543 [02:46<00:00, 1755.95frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__21_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 604\n",
            "\n",
            "[42/44] üé¨ mit_6_034__22_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_23_XPEJg_6Cg6o.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_23_XPEJg_6Cg6o.mkv -> mit_6_034__22_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 297018/297018 [02:54<00:00, 1704.65frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__22_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 940\n",
            "\n",
            "[43/44] üé¨ mit_6_034__23_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_24_iusTmgQyZ44.mkv\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_24_iusTmgQyZ44.mkv -> mit_6_034__23_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 281769/281769 [03:19<00:00, 1409.01frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__23_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 1098\n",
            "\n",
            "[44/44] üé¨ mit_6_034__24_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_25_Tl_p5pgBsyM.mp4\n",
            "   üéôÔ∏è  Extracting audio: MIT 6.034 Artificial Intelligence, Fall 2010_25_Tl_p5pgBsyM.mp4 -> mit_6_034__24_mit_6034_artificial_intelligence_fall_20.wav\n",
            "   üß† Running Whisper transcription...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 311565/311565 [03:42<00:00, 1402.01frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Saved transcript: mit_6_034__24_mit_6034_artificial_intelligence_fall_20.json\n",
            "   ‚Ü™ Segments: 1137\n",
            "\n",
            "‚úÖ CELL 5 complete ‚Äî transcripts generated (or skipped if existing).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CELL 6 ‚Äî Extract Keyframes from Lectures\n",
        "# ----------------------------------------------\n",
        "# Uses:\n",
        "#   - processed/manifests/video_manifest.jsonl\n",
        "#\n",
        "# Produces:\n",
        "#   - processed/keyframes/{video_id}/frame_000001.jpg, frame_000002.jpg, ...\n",
        "#   - processed/manifests/keyframes_manifest.jsonl\n",
        "#\n",
        "# Design:\n",
        "#   - Sample 1 frame every KEYFRAME_EVERY_SEC seconds (we use 5s).\n",
        "#   - For each frame we store:\n",
        "#       video_id, frame_id, image_path, approx_timestamp_sec, index_in_video\n",
        "#\n",
        "# Resume behavior:\n",
        "#   - If processed/keyframes/{video_id} already has frame_*.jpg,\n",
        "#     we SKIP extraction for that video (but still write manifest entries).\n",
        "# ==============================================\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "\n",
        "VIDEO_MANIFEST_PATH = PROJECT_ROOT / \"processed\" / \"manifests\" / \"video_manifest.jsonl\"\n",
        "KEYFRAMES_ROOT = PROJECT_ROOT / \"processed\" / \"keyframes\"\n",
        "KEYFRAMES_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "KEYFRAMES_MANIFEST_PATH = PROJECT_ROOT / \"processed\" / \"manifests\" / \"keyframes_manifest.jsonl\"\n",
        "\n",
        "# We agreed on 5 seconds between keyframes\n",
        "KEYFRAME_EVERY_SEC = 5\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load video manifest\n",
        "# -------------------------------\n",
        "if not VIDEO_MANIFEST_PATH.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå video_manifest.jsonl not found at {VIDEO_MANIFEST_PATH}\")\n",
        "\n",
        "video_records = []\n",
        "with VIDEO_MANIFEST_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        video_records.append(json.loads(line))\n",
        "\n",
        "print(f\"üé¨ Loaded {len(video_records)} videos from manifest.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Helper: extract frames with ffmpeg\n",
        "# -------------------------------\n",
        "def extract_keyframes_for_video(video_path: Path, out_dir: Path, every_sec: int):\n",
        "    \"\"\"\n",
        "    Use ffmpeg to sample 1 frame every 'every_sec' seconds.\n",
        "    Saves frames as: frame_000001.jpg, frame_000002.jpg, ...\n",
        "    If frames already exist, we skip re-extraction for this video.\n",
        "    \"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    existing = list(out_dir.glob(\"frame_*.jpg\"))\n",
        "    if existing:\n",
        "        print(f\"   ‚è© {out_dir.name}: keyframes already exist ({len(existing)} frames)\")\n",
        "        return\n",
        "\n",
        "    print(f\"   üñºÔ∏è  Extracting keyframes into: {out_dir}\")\n",
        "    # fps=1/every_sec ‚Üí 1 frame every N seconds\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\n",
        "        \"-y\",\n",
        "        \"-i\", str(video_path),\n",
        "        \"-vf\", f\"fps=1/{every_sec}\",\n",
        "        \"-qscale:v\", \"2\",  # JPEG quality (2 = high)\n",
        "        str(out_dir / \"frame_%06d.jpg\"),\n",
        "    ]\n",
        "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(\"   ‚ùå ffmpeg failed for\", video_path.name)\n",
        "        print(\"   STDERR (truncated):\", result.stderr[:400])\n",
        "        raise RuntimeError(f\"ffmpeg failed for {video_path}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Helper: natural sort frames\n",
        "# -------------------------------\n",
        "def natural_sort_key(path_obj: Path):\n",
        "    # Sort frame_000001.jpg, frame_000010.jpg numerically by the number\n",
        "    s = path_obj.name\n",
        "    m = re.search(r\"(\\d+)\", s)\n",
        "    if m:\n",
        "        return int(m.group(1))\n",
        "    return s\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Main loop: per-video extraction + manifest build\n",
        "# -------------------------------\n",
        "manifest_entries = []\n",
        "\n",
        "for i, rec in enumerate(video_records, start=1):\n",
        "    video_id = rec[\"video_id\"]\n",
        "    rel_path = rec[\"file_path\"]\n",
        "    video_path = PROJECT_ROOT / rel_path\n",
        "\n",
        "    print(f\"\\n[{i}/{len(video_records)}] üé• {video_id}\")\n",
        "    print(\"   Video file:\", video_path)\n",
        "\n",
        "    if not video_path.exists():\n",
        "        print(\"   ‚ùå Video missing on Drive, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Folder for this video's keyframes\n",
        "    video_keyframe_dir = KEYFRAMES_ROOT / video_id\n",
        "\n",
        "    # 4.1 Extract frames (or skip if already there)\n",
        "    extract_keyframes_for_video(video_path, video_keyframe_dir, KEYFRAME_EVERY_SEC)\n",
        "\n",
        "    # 4.2 Enumerate frames and create manifest rows\n",
        "    frames = sorted(video_keyframe_dir.glob(\"frame_*.jpg\"), key=natural_sort_key)\n",
        "    if not frames:\n",
        "        print(\"   ‚ö†Ô∏è No frames found after extraction, continuing.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"   ‚úì Found {len(frames)} keyframes.\")\n",
        "\n",
        "    for idx, frame_path in enumerate(frames, start=1):\n",
        "        # Approximate timestamp based on sampling interval\n",
        "        timestamp_sec = (idx - 1) * KEYFRAME_EVERY_SEC\n",
        "\n",
        "        frame_id = f\"{video_id}_frame_{idx:06d}\"\n",
        "        rel_img_path = frame_path.relative_to(PROJECT_ROOT)\n",
        "\n",
        "        entry = {\n",
        "            \"video_id\": video_id,\n",
        "            \"frame_id\": frame_id,\n",
        "            \"image_path\": str(rel_img_path),\n",
        "            \"index_in_video\": idx,\n",
        "            \"approx_timestamp_sec\": timestamp_sec,\n",
        "        }\n",
        "        manifest_entries.append(entry)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Write keyframes_manifest.jsonl\n",
        "# -------------------------------\n",
        "with KEYFRAMES_MANIFEST_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for e in manifest_entries:\n",
        "        f.write(json.dumps(e) + \"\\n\")\n",
        "\n",
        "print(f\"\\nüñºÔ∏è Wrote keyframes manifest: {KEYFRAMES_MANIFEST_PATH}\")\n",
        "print(f\"   Total keyframes indexed: {len(manifest_entries)}\")\n",
        "print(\"\\n‚úÖ CELL 6 complete ‚Äî keyframes extracted & indexed.\")\n"
      ],
      "metadata": {
        "id": "55mdexHPNrN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6894b0f-ee3d-4a10-9516-7048b023d9d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé¨ Loaded 44 videos from manifest.\n",
            "\n",
            "[1/44] üé• cs229__01_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_01_jGwO_UgTS7I.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__01_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 904 keyframes.\n",
            "\n",
            "[2/44] üé• cs229__02_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_02_4b4MUYve_U8.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__02_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 939 keyframes.\n",
            "\n",
            "[3/44] üé• cs229__03_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_03_het9HFqo1TQ.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__03_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 955 keyframes.\n",
            "\n",
            "[4/44] üé• cs229__04_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_04_iZTeva0WSTQ.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__04_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 984 keyframes.\n",
            "\n",
            "[5/44] üé• cs229__05_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_05_nt63k3bfXS0.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__05_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 946 keyframes.\n",
            "\n",
            "[6/44] üé• cs229__06_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_06_lDwow4aOrtg.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__06_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 971 keyframes.\n",
            "\n",
            "[7/44] üé• cs229__07_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_07_8NYoQiRANpg.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__07_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 965 keyframes.\n",
            "\n",
            "[8/44] üé• cs229__08_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_08_rjbkWSTjHzM.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__08_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 1001 keyframes.\n",
            "\n",
            "[9/44] üé• cs229__09_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_09_iVOxMcumR4A.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__09_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 1032 keyframes.\n",
            "\n",
            "[10/44] üé• cs229__10_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_10_wr9gUr-eWdA.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__10_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 968 keyframes.\n",
            "\n",
            "[11/44] üé• cs229__11_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_11_MfIjxPh6Pys.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__11_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 963 keyframes.\n",
            "\n",
            "[12/44] üé• cs229__12_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_12_zUazLXZZA2U.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__12_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 919 keyframes.\n",
            "\n",
            "[13/44] üé• cs229__13_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_13_ORrStCArmP4.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__13_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 947 keyframes.\n",
            "\n",
            "[14/44] üé• cs229__14_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_14_rVfZHWTwXSA.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__14_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 966 keyframes.\n",
            "\n",
            "[15/44] üé• cs229__15_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_15_tw6cmL5STuY.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__15_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 957 keyframes.\n",
            "\n",
            "[16/44] üé• cs229__16_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_16_YQA9lLdLig8.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__16_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 938 keyframes.\n",
            "\n",
            "[17/44] üé• cs229__17_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_17_d5gaWTo6kDM.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__17_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 951 keyframes.\n",
            "\n",
            "[18/44] üé• cs229__18_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_18_QFu5nuc-S0s.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__18_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 963 keyframes.\n",
            "\n",
            "[19/44] üé• cs229__19_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_19_0rt2CsEQv6U.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__19_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 973 keyframes.\n",
            "\n",
            "[20/44] üé• cs229__20_stanford_cs229_machine_learning_full_cou\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/CS229/Stanford CS229Ôºö Machine Learning Full Course taught by Andrew Ng ÔΩú Autumn 2018_20_pLhPQynL0tY.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/cs229__20_stanford_cs229_machine_learning_full_cou\n",
            "   ‚úì Found 873 keyframes.\n",
            "\n",
            "[21/44] üé• mit_6_034__01_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_01_TjZBTDzGeGg.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__01_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 568 keyframes.\n",
            "\n",
            "[22/44] üé• mit_6_034__02_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_03_leXa7EKUPFk.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__02_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 599 keyframes.\n",
            "\n",
            "[23/44] üé• mit_6_034__03_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_04_j1H3jAAGlEA.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__03_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 584 keyframes.\n",
            "\n",
            "[24/44] üé• mit_6_034__04_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_05_gGQ-vAmdAOI.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__04_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 583 keyframes.\n",
            "\n",
            "[25/44] üé• mit_6_034__05_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_06_STjW3eH0Cik.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__05_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 579 keyframes.\n",
            "\n",
            "[26/44] üé• mit_6_034__06_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_07_l-tzjenXrvI.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__06_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 590 keyframes.\n",
            "\n",
            "[27/44] üé• mit_6_034__07_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_08_dARl_gGrS4o.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__07_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 541 keyframes.\n",
            "\n",
            "[28/44] üé• mit_6_034__08_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_09_gvmfbePC2pc.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__08_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 618 keyframes.\n",
            "\n",
            "[29/44] üé• mit_6_034__09_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_10_09mb78oiPkA.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__09_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 599 keyframes.\n",
            "\n",
            "[30/44] üé• mit_6_034__10_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_11_SXBG3RGr_Rc.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__10_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 595 keyframes.\n",
            "\n",
            "[31/44] üé• mit_6_034__11_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_12_uXt8qF2Zzfo.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__11_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 608 keyframes.\n",
            "\n",
            "[32/44] üé• mit_6_034__12_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_13_VrMHA3yX_QI.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__12_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 589 keyframes.\n",
            "\n",
            "[33/44] üé• mit_6_034__13_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_14_kHyNqSnzP8Y.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__13_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 567 keyframes.\n",
            "\n",
            "[34/44] üé• mit_6_034__14_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_15_L73hY1pBcQI.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__14_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 574 keyframes.\n",
            "\n",
            "[35/44] üé• mit_6_034__15_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_16_sh3EPjhhd40.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__15_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 563 keyframes.\n",
            "\n",
            "[36/44] üé• mit_6_034__16_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_17__PwhiWxHK8o.webm\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__16_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 595 keyframes.\n",
            "\n",
            "[37/44] üé• mit_6_034__17_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_18_UHBmv7qCey4.mp4\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__17_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 620 keyframes.\n",
            "\n",
            "[38/44] üé• mit_6_034__18_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_19_bQI0OmJPby4.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__18_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 588 keyframes.\n",
            "\n",
            "[39/44] üé• mit_6_034__19_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_20_PimSbFGrwXM.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__19_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 589 keyframes.\n",
            "\n",
            "[40/44] üé• mit_6_034__20_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_21_A6Ud6oUCRak.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__20_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 582 keyframes.\n",
            "\n",
            "[41/44] üé• mit_6_034__21_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_22_EC6bf8JCpDQ.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__21_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 585 keyframes.\n",
            "\n",
            "[42/44] üé• mit_6_034__22_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_23_XPEJg_6Cg6o.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__22_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 594 keyframes.\n",
            "\n",
            "[43/44] üé• mit_6_034__23_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_24_iusTmgQyZ44.mkv\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__23_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 564 keyframes.\n",
            "\n",
            "[44/44] üé• mit_6_034__24_mit_6034_artificial_intelligence_fall_20\n",
            "   Video file: /content/drive/MyDrive/UNISEARCH_MASTER/raw/videos/MIT_6_034/MIT 6.034 Artificial Intelligence, Fall 2010_25_Tl_p5pgBsyM.mp4\n",
            "   üñºÔ∏è  Extracting keyframes into: /content/drive/MyDrive/UNISEARCH_MASTER/processed/keyframes/mit_6_034__24_mit_6034_artificial_intelligence_fall_20\n",
            "   ‚úì Found 623 keyframes.\n",
            "\n",
            "üñºÔ∏è Wrote keyframes manifest: /content/drive/MyDrive/UNISEARCH_MASTER/processed/manifests/keyframes_manifest.jsonl\n",
            "   Total keyframes indexed: 33212\n",
            "\n",
            "‚úÖ CELL 6 complete ‚Äî keyframes extracted & indexed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CELL 7 ‚Äî Align Keyframes with Transcripts\n",
        "# ----------------------------------------------\n",
        "# Uses:\n",
        "#   - processed/manifests/keyframes_manifest.jsonl\n",
        "#   - processed/transcripts/{video_id}.json  (from Whisper)\n",
        "#\n",
        "# Produces:\n",
        "#   - processed/manifests/aligned_keyframes_with_snippets.jsonl\n",
        "#\n",
        "# For each keyframe (with approx_timestamp_sec = T), we gather transcript\n",
        "# segments that overlap [T - WINDOW_SEC, T + WINDOW_SEC], concatenate them\n",
        "# into a readable snippet, and save one JSONL row per keyframe:\n",
        "#\n",
        "# {\n",
        "#   \"video_id\": \"...\",\n",
        "#   \"frame_id\": \"...\",\n",
        "#   \"image_path\": \"processed/keyframes/.../frame_000123.jpg\",\n",
        "#   \"approx_timestamp_sec\": 180,\n",
        "#   \"transcript_snippet\": \"Professor explains ...\",\n",
        "#   \"segment_ids\": [23, 24]\n",
        "# }\n",
        "#\n",
        "# You can safely re-run this cell: it rebuilds the aligned manifest based\n",
        "# on whatever transcripts exist at the moment (great while Whisper is still running).\n",
        "# ==============================================\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import bisect\n",
        "import re\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "TRANSCRIPTS_DIR = PROJECT_ROOT / \"processed\" / \"transcripts\"\n",
        "MANIFEST_DIR = PROJECT_ROOT / \"processed\" / \"manifests\"\n",
        "KEYFRAMES_MANIFEST_PATH = MANIFEST_DIR / \"keyframes_manifest.jsonl\"\n",
        "ALIGNED_OUT_PATH = MANIFEST_DIR / \"aligned_keyframes_with_snippets.jsonl\"\n",
        "\n",
        "# Alignment knobs\n",
        "WINDOW_SEC = 7.5          # gather transcript text from [T - WINDOW_SEC, T + WINDOW_SEC]\n",
        "SNIPPET_MAX_CHARS = 420   # trim very long snippets for readability\n",
        "MIN_CHARS_TO_KEEP = 60    # if concatenated text is shorter, keep it as-is\n",
        "\n",
        "# -------------------------------\n",
        "# Helpers\n",
        "# -------------------------------\n",
        "def load_json(path: Path):\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_jsonl(path: Path):\n",
        "    data = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "def clean_spaces(s: str) -> str:\n",
        "    # collapse whitespace, keep punctuation spacing readable\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def build_snippet(segments, t_center: float, window: float):\n",
        "    \"\"\"\n",
        "    Given a video's transcript 'segments' (list of dicts with 'start','end','text'),\n",
        "    collect text overlapping [t_center - window, t_center + window].\n",
        "    If nothing overlaps (rare), fall back to the single nearest segment by start time.\n",
        "    \"\"\"\n",
        "    t0, t1 = t_center - window, t_center + window\n",
        "\n",
        "    selected = []\n",
        "    for seg in segments:\n",
        "        s = float(seg.get(\"start\", 0.0))\n",
        "        e = float(seg.get(\"end\", 0.0))\n",
        "        if e >= t0 and s <= t1:  # overlap\n",
        "            selected.append(seg)\n",
        "\n",
        "    # Fallback: nearest segment by start time (if no overlap)\n",
        "    if not selected and segments:\n",
        "        starts = [float(seg.get(\"start\", 0.0)) for seg in segments]\n",
        "        i = bisect.bisect_left(starts, t_center)\n",
        "        # pick closest among i-1 and i\n",
        "        cand = []\n",
        "        if 0 <= i < len(segments):\n",
        "            cand.append(segments[i])\n",
        "        if 0 <= i-1 < len(segments):\n",
        "            cand.append(segments[i-1])\n",
        "        # choose the closer start time\n",
        "        if cand:\n",
        "            selected = [min(cand, key=lambda s: abs(float(s.get(\"start\", 0.0)) - t_center))]\n",
        "\n",
        "    if not selected:\n",
        "        return \"\", []\n",
        "\n",
        "    # Concatenate texts in chronological order\n",
        "    selected = sorted(selected, key=lambda x: float(x.get(\"start\", 0.0)))\n",
        "    txt = \" \".join(clean_spaces(seg.get(\"text\", \"\")) for seg in selected if seg.get(\"text\"))\n",
        "    txt = clean_spaces(txt)\n",
        "\n",
        "    # Trim very long snippets, but avoid cutting mid-sentence harshly\n",
        "    if len(txt) > SNIPPET_MAX_CHARS:\n",
        "        # try to cut at a period near the limit for nicer UX\n",
        "        cut = txt.rfind(\".\", 0, SNIPPET_MAX_CHARS)\n",
        "        if cut == -1 or cut < MIN_CHARS_TO_KEEP:\n",
        "            cut = SNIPPET_MAX_CHARS\n",
        "        txt = txt[:cut].rstrip() + \" ‚Ä¶\"\n",
        "\n",
        "    seg_ids = [seg.get(\"id\") for seg in selected if \"id\" in seg]\n",
        "    return txt, seg_ids\n",
        "\n",
        "# -------------------------------\n",
        "# Load inputs\n",
        "# -------------------------------\n",
        "if not KEYFRAMES_MANIFEST_PATH.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå keyframes_manifest.jsonl not found at {KEYFRAMES_MANIFEST_PATH}\")\n",
        "\n",
        "keyframes = load_jsonl(KEYFRAMES_MANIFEST_PATH)\n",
        "print(f\"üñºÔ∏è Loaded keyframes: {len(keyframes)}\")\n",
        "\n",
        "# Preload transcript index: video_id -> {\"segments\": [...], \"ok\": bool}\n",
        "transcripts = {}\n",
        "missing_videos = set()\n",
        "\n",
        "# We'll lazily load transcripts per video_id on demand to keep memory reasonable\n",
        "def get_transcript(video_id: str):\n",
        "    if video_id in transcripts:\n",
        "        return transcripts[video_id]\n",
        "\n",
        "    t_path = TRANSCRIPTS_DIR / f\"{video_id}.json\"\n",
        "    if not t_path.exists():\n",
        "        transcripts[video_id] = {\"segments\": [], \"ok\": False}\n",
        "        missing_videos.add(video_id)\n",
        "        return transcripts[video_id]\n",
        "\n",
        "    data = load_json(t_path)\n",
        "    segs = data.get(\"segments\", [])\n",
        "    # ensure numeric times & clean text\n",
        "    norm = []\n",
        "    for seg in segs:\n",
        "        s = float(seg.get(\"start\", 0.0))\n",
        "        e = float(seg.get(\"end\", s))\n",
        "        text = clean_spaces(seg.get(\"text\", \"\"))\n",
        "        norm.append({\"id\": seg.get(\"id\"), \"start\": s, \"end\": e, \"text\": text})\n",
        "    # sort by start (Whisper usually already sorted)\n",
        "    norm.sort(key=lambda x: x[\"start\"])\n",
        "\n",
        "    transcripts[video_id] = {\"segments\": norm, \"ok\": True}\n",
        "    return transcripts[video_id]\n",
        "\n",
        "# -------------------------------\n",
        "# Align all keyframes\n",
        "# -------------------------------\n",
        "aligned_rows = []\n",
        "count_no_transcript = 0\n",
        "count_no_snippet = 0\n",
        "\n",
        "for i, kf in enumerate(keyframes, start=1):\n",
        "    if i % 2000 == 0:\n",
        "        print(f\"   ‚Ä¶ aligned {i} keyframes so far\")\n",
        "\n",
        "    video_id = kf[\"video_id\"]\n",
        "    t = float(kf.get(\"approx_timestamp_sec\", 0.0))\n",
        "\n",
        "    tr = get_transcript(video_id)\n",
        "    if not tr[\"ok\"]:\n",
        "        count_no_transcript += 1\n",
        "        continue\n",
        "\n",
        "    snippet, seg_ids = build_snippet(tr[\"segments\"], t_center=t, window=WINDOW_SEC)\n",
        "    if not snippet:\n",
        "        count_no_snippet += 1\n",
        "        # We still write the row (without snippet) so downstream can decide to filter or backfill later\n",
        "        # If you prefer to skip such rows entirely, just \"continue\" here.\n",
        "\n",
        "    aligned_rows.append({\n",
        "        \"video_id\": video_id,\n",
        "        \"frame_id\": kf[\"frame_id\"],\n",
        "        \"image_path\": kf[\"image_path\"],\n",
        "        \"approx_timestamp_sec\": t,\n",
        "        \"transcript_snippet\": snippet,\n",
        "        \"segment_ids\": seg_ids,\n",
        "        \"window_sec\": WINDOW_SEC\n",
        "    })\n",
        "\n",
        "# -------------------------------\n",
        "# Write output\n",
        "# -------------------------------\n",
        "with ALIGNED_OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for row in aligned_rows:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(f\"\\nüß© Wrote aligned manifest: {ALIGNED_OUT_PATH}\")\n",
        "print(f\"   Total keyframes processed : {len(keyframes)}\")\n",
        "print(f\"   Aligned rows written      : {len(aligned_rows)}\")\n",
        "print(f\"   Missing transcripts (kf)  : {count_no_transcript}\")\n",
        "print(f\"   No-snippet rows (fallback): {count_no_snippet}\")\n",
        "\n",
        "if missing_videos:\n",
        "    sample = list(sorted(missing_videos))[:5]\n",
        "    print(f\"\\n‚ÑπÔ∏è Transcripts missing for {len(missing_videos)} videos (showing up to 5):\")\n",
        "    for v in sample:\n",
        "        print(\"   -\", v)\n",
        "print(\"\\n‚úÖ CELL 7 complete ‚Äî keyframes linked to transcript snippets.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c93IB8ATJ1wL",
        "outputId": "4fcb61f0-ebd8-4471-d8e4-fed271f7740b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Loaded keyframes: 33212\n",
            "   ‚Ä¶ aligned 2000 keyframes so far\n",
            "   ‚Ä¶ aligned 4000 keyframes so far\n",
            "   ‚Ä¶ aligned 6000 keyframes so far\n",
            "   ‚Ä¶ aligned 8000 keyframes so far\n",
            "   ‚Ä¶ aligned 10000 keyframes so far\n",
            "   ‚Ä¶ aligned 12000 keyframes so far\n",
            "   ‚Ä¶ aligned 14000 keyframes so far\n",
            "   ‚Ä¶ aligned 16000 keyframes so far\n",
            "   ‚Ä¶ aligned 18000 keyframes so far\n",
            "   ‚Ä¶ aligned 20000 keyframes so far\n",
            "   ‚Ä¶ aligned 22000 keyframes so far\n",
            "   ‚Ä¶ aligned 24000 keyframes so far\n",
            "   ‚Ä¶ aligned 26000 keyframes so far\n",
            "   ‚Ä¶ aligned 28000 keyframes so far\n",
            "   ‚Ä¶ aligned 30000 keyframes so far\n",
            "   ‚Ä¶ aligned 32000 keyframes so far\n",
            "\n",
            "üß© Wrote aligned manifest: /content/drive/MyDrive/UNISEARCH_MASTER/processed/manifests/aligned_keyframes_with_snippets.jsonl\n",
            "   Total keyframes processed : 33212\n",
            "   Aligned rows written      : 33212\n",
            "   Missing transcripts (kf)  : 0\n",
            "   No-snippet rows (fallback): 0\n",
            "\n",
            "‚úÖ CELL 7 complete ‚Äî keyframes linked to transcript snippets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7.1 ‚Äî Alignment sanity check\n",
        "from pathlib import Path\n",
        "import json, itertools\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "KF_MANIFEST = PROJECT_ROOT / \"processed\" / \"manifests\" / \"keyframes_manifest.jsonl\"\n",
        "ALIGNED = PROJECT_ROOT / \"processed\" / \"manifests\" / \"aligned_keyframes_with_snippets.jsonl\"\n",
        "\n",
        "def load_jsonl(p):\n",
        "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if line: yield json.loads(line)\n",
        "\n",
        "kfs_total = sum(1 for _ in load_jsonl(KF_MANIFEST))\n",
        "aligned = list(load_jsonl(ALIGNED))\n",
        "with_snip = sum(1 for r in aligned if r.get(\"transcript_snippet\"))\n",
        "no_snip  = len(aligned) - with_snip\n",
        "\n",
        "print(f\"Keyframes total (from KF manifest): {kfs_total}\")\n",
        "print(f\"Aligned rows written              : {len(aligned)}\")\n",
        "print(f\" ‚îú‚îÄ with snippet                   : {with_snip}\")\n",
        "print(f\" ‚îî‚îÄ without snippet                : {no_snip}\")\n",
        "\n",
        "# show a few sample aligned rows for eyeballing\n",
        "print(\"\\nSample aligned rows:\")\n",
        "for r in itertools.islice((r for r in aligned if r.get('transcript_snippet')), 3):\n",
        "    print({\n",
        "        \"video_id\": r[\"video_id\"],\n",
        "        \"frame_id\": r[\"frame_id\"],\n",
        "        \"t\": r[\"approx_timestamp_sec\"],\n",
        "        \"snippet_preview\": (r[\"transcript_snippet\"][:140] + \"‚Ä¶\") if len(r[\"transcript_snippet\"])>140 else r[\"transcript_snippet\"]\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqiIr1KnkbPe",
        "outputId": "85ea7342-0b6c-43a4-f8a6-32a55e21a7cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyframes total (from KF manifest): 33212\n",
            "Aligned rows written              : 33212\n",
            " ‚îú‚îÄ with snippet                   : 33212\n",
            " ‚îî‚îÄ without snippet                : 0\n",
            "\n",
            "Sample aligned rows:\n",
            "{'video_id': 'cs229__01_stanford_cs229_machine_learning_full_cou', 'frame_id': 'cs229__01_stanford_cs229_machine_learning_full_cou_frame_000001', 't': 0.0, 'snippet_preview': 'Welcome to CS229 Machine Learning. Uh, some of you know that this is a class that taught at Stanford for a long time,'}\n",
            "{'video_id': 'cs229__01_stanford_cs229_machine_learning_full_cou', 'frame_id': 'cs229__01_stanford_cs229_machine_learning_full_cou_frame_000002', 't': 5.0, 'snippet_preview': 'Welcome to CS229 Machine Learning. Uh, some of you know that this is a class that taught at Stanford for a long time, and this is often the ‚Ä¶'}\n",
            "{'video_id': 'cs229__01_stanford_cs229_machine_learning_full_cou', 'frame_id': 'cs229__01_stanford_cs229_machine_learning_full_cou_frame_000003', 't': 10.0, 'snippet_preview': 'Welcome to CS229 Machine Learning. Uh, some of you know that this is a class that taught at Stanford for a long time, and this is often the ‚Ä¶'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CELL 8 ‚Äî Build Lecture Passages from Transcripts\n",
        "# ----------------------------------------------\n",
        "# Uses:\n",
        "#   - processed/transcripts/{video_id}.json\n",
        "#   - processed/manifests/video_manifest.jsonl\n",
        "#\n",
        "# Produces:\n",
        "#   - processed/manifests/lecture_passages.jsonl\n",
        "#\n",
        "# Design:\n",
        "#   - Concatenate transcript segments for each video in order\n",
        "#   - Create overlapping text chunks (by characters) for retrieval\n",
        "#   - Keep timing: start_sec/end_sec from covered segments\n",
        "#   - Fields per row:\n",
        "#       {\n",
        "#         \"source_type\": \"lecture\",\n",
        "#         \"video_id\": \"...\",\n",
        "#         \"course\": \"CS229\",\n",
        "#         \"video_index\": 7,\n",
        "#         \"chunk_id\": \"cs229__07_...__chunk_0001\",\n",
        "#         \"start_sec\": 120.3,\n",
        "#         \"end_sec\": 245.1,\n",
        "#         \"text\": \"cleaned text ...\",\n",
        "#       }\n",
        "# ==============================================\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "TRANSCRIPTS_DIR = PROJECT_ROOT / \"processed\" / \"transcripts\"\n",
        "MANIFEST_DIR = PROJECT_ROOT / \"processed\" / \"manifests\"\n",
        "VIDEO_MANIFEST_PATH = MANIFEST_DIR / \"video_manifest.jsonl\"\n",
        "LECTURE_PASSAGES_PATH = MANIFEST_DIR / \"lecture_passages.jsonl\"\n",
        "\n",
        "# Chunking knobs (tune if you like)\n",
        "CHUNK_CHARS = 1200     # target chunk length (characters)\n",
        "CHUNK_OVERLAP = 600    # overlap between chunks (characters)\n",
        "MIN_KEEP_CHARS = 200   # drop tiny crumbs below this\n",
        "\n",
        "# -------------------------------\n",
        "# Helpers\n",
        "# -------------------------------\n",
        "def load_json(path: Path):\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_jsonl(path: Path):\n",
        "    out = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if line:\n",
        "                out.append(json.loads(line))\n",
        "    return out\n",
        "\n",
        "def clean_spaces(s: str) -> str:\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
        "    s = re.sub(r\"\\n\\s+\", \"\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def merge_segments(segments):\n",
        "    \"\"\"\n",
        "    Return a list of tokens with (text, start, end).\n",
        "    We'll chunk over the concatenated text but keep per-segment timing,\n",
        "    so we can compute chunk start/end from covered segments.\n",
        "    \"\"\"\n",
        "    merged = []\n",
        "    for seg in segments:\n",
        "        txt = seg.get(\"text\") or \"\"\n",
        "        if not txt.strip():\n",
        "            continue\n",
        "        merged.append({\n",
        "            \"text\": clean_spaces(txt),\n",
        "            \"start\": float(seg.get(\"start\", 0.0)),\n",
        "            \"end\": float(seg.get(\"end\", 0.0)),\n",
        "        })\n",
        "    return merged\n",
        "\n",
        "def build_chunks(merged_segs, video_prefix):\n",
        "    \"\"\"\n",
        "    Chunk by character length with overlap, but align boundaries to segment edges\n",
        "    so each chunk's start/end_sec come from covered segments.\n",
        "    \"\"\"\n",
        "    # Build a flat string with segment boundaries recorded\n",
        "    parts = []\n",
        "    offsets = []  # [(global_char_start, global_char_end, seg_index)]\n",
        "    total = 0\n",
        "    for i, seg in enumerate(merged_segs):\n",
        "        t = seg[\"text\"]\n",
        "        if parts:\n",
        "            parts.append(\" \")  # ensure space between segments\n",
        "            total += 1\n",
        "        start_off = total\n",
        "        parts.append(t)\n",
        "        total += len(t)\n",
        "        end_off = total\n",
        "        offsets.append((start_off, end_off, i))\n",
        "    full_text = \"\".join(parts)\n",
        "\n",
        "    chunks = []\n",
        "    if not full_text.strip():\n",
        "        return chunks\n",
        "\n",
        "    L = len(full_text)\n",
        "    cursor = 0\n",
        "    chunk_idx = 0\n",
        "\n",
        "    while cursor < L:\n",
        "        # target window\n",
        "        end = min(L, cursor + CHUNK_CHARS)\n",
        "\n",
        "        # try to end at a sentence boundary near the target end\n",
        "        window_text = full_text[cursor:end]\n",
        "        cut = window_text.rfind(\". \")\n",
        "        if cut != -1 and (end - (cursor + cut + 1)) <= 200 and (cut + 1) >= MIN_KEEP_CHARS:\n",
        "            end = cursor + cut + 1  # cut after period\n",
        "\n",
        "        # determine covered segments using offsets\n",
        "        covered = [ix for (s,e,ix) in offsets if not (e <= cursor or s >= end)]\n",
        "        if not covered:\n",
        "            # extend slightly to capture at least one segment\n",
        "            # or break if we really can't (shouldn't happen often)\n",
        "            # move cursor forward safely\n",
        "            cursor = min(L, cursor + CHUNK_CHARS - CHUNK_OVERLAP)\n",
        "            if cursor >= L:\n",
        "                break\n",
        "            continue\n",
        "\n",
        "        seg_start_ix = min(covered)\n",
        "        seg_end_ix = max(covered)\n",
        "        start_sec = merged_segs[seg_start_ix][\"start\"]\n",
        "        end_sec   = merged_segs[seg_end_ix][\"end\"]\n",
        "\n",
        "        chunk_text = clean_spaces(full_text[cursor:end])\n",
        "        if len(chunk_text) >= MIN_KEEP_CHARS:\n",
        "            chunk_idx += 1\n",
        "            chunks.append({\n",
        "                \"chunk_id\": f\"{video_prefix}__chunk_{chunk_idx:04d}\",\n",
        "                \"start_sec\": float(start_sec),\n",
        "                \"end_sec\": float(end_sec),\n",
        "                \"text\": chunk_text\n",
        "            })\n",
        "\n",
        "        if end >= L:\n",
        "            break\n",
        "        # move cursor forward with overlap\n",
        "        cursor = max(end - CHUNK_OVERLAP, cursor + 1)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# -------------------------------\n",
        "# Load video manifest for course/meta\n",
        "# -------------------------------\n",
        "if not VIDEO_MANIFEST_PATH.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Missing {VIDEO_MANIFEST_PATH}\")\n",
        "\n",
        "video_manifest = load_jsonl(VIDEO_MANIFEST_PATH)\n",
        "video_meta = {}  # video_id -> {\"course\":..., \"index_in_course\":...}\n",
        "for rec in video_manifest:\n",
        "    video_meta[rec[\"video_id\"]] = {\n",
        "        \"course\": rec.get(\"course\"),\n",
        "        \"video_index\": rec.get(\"index_in_course\")\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# Iterate transcripts and create passages\n",
        "# -------------------------------\n",
        "written = 0\n",
        "videos_done = 0\n",
        "videos_skipped = 0\n",
        "\n",
        "with LECTURE_PASSAGES_PATH.open(\"w\", encoding=\"utf-8\") as out_f:\n",
        "    for video_id, meta in video_meta.items():\n",
        "        t_path = TRANSCRIPTS_DIR / f\"{video_id}.json\"\n",
        "        if not t_path.exists():\n",
        "            videos_skipped += 1\n",
        "            continue\n",
        "\n",
        "        data = load_json(t_path)\n",
        "        segs = data.get(\"segments\", [])\n",
        "        # normalize + sort segments\n",
        "        norm = []\n",
        "        for i, s in enumerate(segs):\n",
        "            txt = s.get(\"text\") or \"\"\n",
        "            if not txt.strip():\n",
        "                continue\n",
        "            norm.append({\n",
        "                \"id\": s.get(\"id\", i),\n",
        "                \"start\": float(s.get(\"start\", 0.0)),\n",
        "                \"end\": float(s.get(\"end\", 0.0)),\n",
        "                \"text\": clean_spaces(txt),\n",
        "            })\n",
        "        norm.sort(key=lambda x: x[\"start\"])\n",
        "\n",
        "        merged = merge_segments(norm)\n",
        "        video_prefix = video_id\n",
        "        chunks = build_chunks(merged, video_prefix)\n",
        "\n",
        "        for c in chunks:\n",
        "            row = {\n",
        "                \"source_type\": \"lecture\",\n",
        "                \"video_id\": video_id,\n",
        "                \"course\": meta.get(\"course\"),\n",
        "                \"video_index\": meta.get(\"video_index\"),\n",
        "                \"chunk_id\": c[\"chunk_id\"],\n",
        "                \"start_sec\": c[\"start_sec\"],\n",
        "                \"end_sec\": c[\"end_sec\"],\n",
        "                \"text\": c[\"text\"],\n",
        "            }\n",
        "            out_f.write(json.dumps(row) + \"\\n\")\n",
        "            written += 1\n",
        "\n",
        "        videos_done += 1\n",
        "\n",
        "print(f\"üßæ Wrote lecture passages: {LECTURE_PASSAGES_PATH}\")\n",
        "print(f\"   Videos with transcripts processed : {videos_done}\")\n",
        "print(f\"   Videos without transcripts        : {videos_skipped}\")\n",
        "print(f\"   Total chunks written              : {written}\")\n",
        "print(\"\\n‚úÖ CELL 8 complete ‚Äî lecture passages ready for embeddings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGz6QCBCkslu",
        "outputId": "bf995c78-bb62-4445-a4d8-ab07918d984a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßæ Wrote lecture passages: /content/drive/MyDrive/UNISEARCH_MASTER/processed/manifests/lecture_passages.jsonl\n",
            "   Videos with transcripts processed : 44\n",
            "   Videos without transcripts        : 0\n",
            "   Total chunks written              : 20624\n",
            "\n",
            "‚úÖ CELL 8 complete ‚Äî lecture passages ready for embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# CELL 9 ‚Äî Build Paper Passages from PDFs\n",
        "# ----------------------------------------------\n",
        "# Uses:\n",
        "#   - raw/papers/*.pdf\n",
        "#   - processed/manifests/paper_manifest.jsonl\n",
        "#\n",
        "# Produces:\n",
        "#   - processed/manifests/paper_passages.jsonl\n",
        "#\n",
        "# Each passage row looks like:\n",
        "# {\n",
        "#   \"source_type\": \"paper\",\n",
        "#   \"paper_id\": \"...\",\n",
        "#   \"file_name\": \"resnet_2015_deep_residual.pdf\",\n",
        "#   \"title\": \"resnet 2015 deep residual\",\n",
        "#   \"chunk_id\": \"paper_031_resnet_2015_deep_residual__chunk_0001\",\n",
        "#   \"page_start\": 0,\n",
        "#   \"page_end\": 1,\n",
        "#   \"text\": \"chunk text ...\"\n",
        "# }\n",
        "#\n",
        "# Design:\n",
        "#   - Extract text from each PDF using PyMuPDF.\n",
        "#   - Concatenate pages but remember page boundaries.\n",
        "#   - Create overlapping character-based chunks, like we did for lectures.\n",
        "#   - For each chunk, record which pages it covers.\n",
        "# ==============================================\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "PAPERS_ROOT = PROJECT_ROOT / \"raw\" / \"papers\"\n",
        "MANIFEST_DIR = PROJECT_ROOT / \"processed\" / \"manifests\"\n",
        "PAPER_MANIFEST_PATH = MANIFEST_DIR / \"paper_manifest.jsonl\"\n",
        "PAPER_PASSAGES_PATH = MANIFEST_DIR / \"paper_passages.jsonl\"\n",
        "\n",
        "# ---------- Install & import PyMuPDF (fitz) ----------\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing PyMuPDF (fitz) for PDF parsing...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymupdf\"])\n",
        "    import fitz\n",
        "\n",
        "\n",
        "# ---------- Chunking knobs (similar to lectures, maybe slightly larger) ----------\n",
        "CHUNK_CHARS = 1400      # target chunk length (characters)\n",
        "CHUNK_OVERLAP = 700     # overlap between chunks\n",
        "MIN_KEEP_CHARS = 250    # drop tiny chunks below this length\n",
        "\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def load_jsonl(path: Path):\n",
        "    out = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                out.append(json.loads(line))\n",
        "    return out\n",
        "\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Light cleanup for PDF text:\n",
        "      - collapse repeated whitespace\n",
        "      - remove very long sequences of dots\n",
        "    \"\"\"\n",
        "    s = s.replace(\"\\u00a0\", \" \")  # non-breaking spaces\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
        "    s = re.sub(r\"\\n\\s+\", \"\\n\", s)\n",
        "    # collapse crazy dot leaders (\"...........\") often found in PDFs\n",
        "    s = re.sub(r\"\\.{4,}\", \"...\", s)\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "def extract_pages(pdf_path: Path):\n",
        "    \"\"\"\n",
        "    Return list of (page_index, page_text) for a PDF.\n",
        "    Skips pages that have almost no text.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc[i]\n",
        "        txt = page.get_text(\"text\")\n",
        "        txt = clean_text(txt)\n",
        "        if len(txt) < 20:\n",
        "            continue\n",
        "        pages.append((i, txt))\n",
        "    doc.close()\n",
        "    return pages\n",
        "\n",
        "\n",
        "def build_chunks_from_pages(pages, chunk_chars, overlap, min_keep):\n",
        "    \"\"\"\n",
        "    Given pages = [(page_index, text), ...],\n",
        "    build overlapping chunks over the concatenated text,\n",
        "    while remembering which pages each chunk spans.\n",
        "    \"\"\"\n",
        "    if not pages:\n",
        "        return []\n",
        "\n",
        "    parts = []\n",
        "    page_offsets = []  # [(start_off, end_off, page_index)]\n",
        "    total = 0\n",
        "\n",
        "    for page_idx, txt in pages:\n",
        "        if parts:\n",
        "            parts.append(\"\\n\\n\")  # separate pages\n",
        "            total += 2\n",
        "        start_off = total\n",
        "        parts.append(txt)\n",
        "        total += len(txt)\n",
        "        end_off = total\n",
        "        page_offsets.append((start_off, end_off, page_idx))\n",
        "\n",
        "    full_text = \"\".join(parts)\n",
        "    full_text = clean_text(full_text)\n",
        "\n",
        "    if not full_text:\n",
        "        return []\n",
        "\n",
        "    chunks = []\n",
        "    L = len(full_text)\n",
        "    cursor = 0\n",
        "    chunk_idx = 0\n",
        "\n",
        "    while cursor < L:\n",
        "        target_end = min(L, cursor + chunk_chars)\n",
        "        window_text = full_text[cursor:target_end]\n",
        "\n",
        "        # Try to break near a sentence boundary close to target_end\n",
        "        cut = window_text.rfind(\". \")\n",
        "        if cut != -1 and (target_end - (cursor + cut + 1)) <= 200 and (cut + 1) >= min_keep:\n",
        "            end = cursor + cut + 1\n",
        "        else:\n",
        "            end = target_end\n",
        "\n",
        "        # Determine which pages this chunk covers\n",
        "        covered_pages = [pidx for (start, stop, pidx) in page_offsets if not (stop <= cursor or start >= end)]\n",
        "        if not covered_pages:\n",
        "            # Move cursor forward and continue\n",
        "            cursor = min(L, cursor + chunk_chars - overlap)\n",
        "            if cursor >= L:\n",
        "                break\n",
        "            continue\n",
        "\n",
        "        page_start = min(covered_pages)\n",
        "        page_end = max(covered_pages)\n",
        "\n",
        "        chunk_text = clean_text(full_text[cursor:end])\n",
        "        if len(chunk_text) >= min_keep:\n",
        "            chunk_idx += 1\n",
        "            chunks.append({\n",
        "                \"chunk_local_index\": chunk_idx,\n",
        "                \"text\": chunk_text,\n",
        "                \"page_start\": int(page_start),\n",
        "                \"page_end\": int(page_end),\n",
        "            })\n",
        "\n",
        "        if end >= L:\n",
        "            break\n",
        "        cursor = max(end - overlap, cursor + 1)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ---------- Main: iterate paper manifest & build passages ----------\n",
        "if not PAPER_MANIFEST_PATH.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå paper_manifest.jsonl not found at {PAPER_MANIFEST_PATH}\")\n",
        "\n",
        "paper_manifest = load_jsonl(PAPER_MANIFEST_PATH)\n",
        "print(f\"üìö Loaded {len(paper_manifest)} papers from manifest.\")\n",
        "\n",
        "total_chunks = 0\n",
        "papers_done = 0\n",
        "papers_skipped = 0\n",
        "\n",
        "with PAPER_PASSAGES_PATH.open(\"w\", encoding=\"utf-8\") as out_f:\n",
        "    for rec in paper_manifest:\n",
        "        paper_id = rec[\"paper_id\"]\n",
        "        file_path = rec[\"file_path\"]   # relative to PROJECT_ROOT\n",
        "        file_name = rec.get(\"file_name\")\n",
        "        title_guess = rec.get(\"title_guess\", \"\")\n",
        "\n",
        "        pdf_path = PROJECT_ROOT / file_path\n",
        "\n",
        "        print(f\"\\nüìÑ [{papers_done + papers_skipped + 1}/{len(paper_manifest)}] {paper_id}\")\n",
        "        print(f\"   File: {pdf_path}\")\n",
        "\n",
        "        if not pdf_path.exists():\n",
        "            print(\"   ‚ùå Missing PDF on Drive, skipping.\")\n",
        "            papers_skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Extract per-page text\n",
        "        try:\n",
        "            pages = extract_pages(pdf_path)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Failed to parse PDF: {e}\")\n",
        "            papers_skipped += 1\n",
        "            continue\n",
        "\n",
        "        if not pages:\n",
        "            print(\"   ‚ö†Ô∏è No usable text extracted, skipping.\")\n",
        "            papers_skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Build overlapping chunks\n",
        "        chunks = build_chunks_from_pages(pages, CHUNK_CHARS, CHUNK_OVERLAP, MIN_KEEP_CHARS)\n",
        "        print(f\"   ‚úì Built {len(chunks)} chunks.\")\n",
        "\n",
        "        for c in chunks:\n",
        "            chunk_id = f\"{paper_id}__chunk_{c['chunk_local_index']:04d}\"\n",
        "            row = {\n",
        "                \"source_type\": \"paper\",\n",
        "                \"paper_id\": paper_id,\n",
        "                \"file_name\": file_name,\n",
        "                \"title\": title_guess,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"page_start\": c[\"page_start\"],\n",
        "                \"page_end\": c[\"page_end\"],\n",
        "                \"text\": c[\"text\"],\n",
        "            }\n",
        "            out_f.write(json.dumps(row) + \"\\n\")\n",
        "            total_chunks += 1\n",
        "\n",
        "        papers_done += 1\n",
        "\n",
        "print(f\"\\nüßæ Wrote paper passages: {PAPER_PASSAGES_PATH}\")\n",
        "print(f\"   Papers processed          : {papers_done}\")\n",
        "print(f\"   Papers skipped            : {papers_skipped}\")\n",
        "print(f\"   Total paper chunks written: {total_chunks}\")\n",
        "print(\"\\n‚úÖ CELL 9 complete ‚Äî paper passages ready for embeddings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJuKZLeioBej",
        "outputId": "925def35-6a28-40c6-f4cf-67c4b6678074"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Loaded 42 papers from manifest.\n",
            "\n",
            "üìÑ [1/42] paper_001_adam_2014\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/adam_2014.pdf\n",
            "   ‚úì Built 66 chunks.\n",
            "\n",
            "üìÑ [2/42] paper_002_albef_2021\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/albef_2021.pdf\n",
            "   ‚úì Built 83 chunks.\n",
            "\n",
            "üìÑ [3/42] paper_003_albert_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/albert_2019.pdf\n",
            "   ‚úì Built 546 chunks.\n",
            "\n",
            "üìÑ [4/42] paper_004_alexnet_2012_imagenet\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/alexnet_2012_imagenet.pdf\n",
            "   ‚úì Built 68 chunks.\n",
            "\n",
            "üìÑ [5/42] paper_005_align_2021\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/align_2021.pdf\n",
            "   ‚úì Built 548 chunks.\n",
            "\n",
            "üìÑ [6/42] paper_006_attention_is_all_you_need_2017\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/attention_is_all_you_need_2017.pdf\n",
            "   ‚úì Built 512 chunks.\n",
            "\n",
            "üìÑ [7/42] paper_007_bahdanau_attention_2014\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/bahdanau_attention_2014.pdf\n",
            "   ‚úì Built 528 chunks.\n",
            "\n",
            "üìÑ [8/42] paper_008_batchnorm_2015\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/batchnorm_2015.pdf\n",
            "   ‚úì Built 69 chunks.\n",
            "\n",
            "üìÑ [9/42] paper_009_bert_2018\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/bert_2018.pdf\n",
            "   ‚úì Built 550 chunks.\n",
            "\n",
            "üìÑ [10/42] paper_010_blip2_2023\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/blip2_2023.pdf\n",
            "   ‚úì Built 527 chunks.\n",
            "\n",
            "üìÑ [11/42] paper_011_blip_2022\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/blip_2022.pdf\n",
            "   ‚úì Built 533 chunks.\n",
            "\n",
            "üìÑ [12/42] paper_012_byol_2020\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/byol_2020.pdf\n",
            "   ‚úì Built 167 chunks.\n",
            "\n",
            "üìÑ [13/42] paper_013_clip_2021\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/clip_2021.pdf\n",
            "   ‚úì Built 797 chunks.\n",
            "\n",
            "üìÑ [14/42] paper_014_cpc_2018\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/cpc_2018.pdf\n",
            "   ‚úì Built 64 chunks.\n",
            "\n",
            "üìÑ [15/42] paper_015_densenet_2016\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/densenet_2016.pdf\n",
            "   ‚úì Built 520 chunks.\n",
            "\n",
            "üìÑ [16/42] paper_016_distilbert_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/distilbert_2019.pdf\n",
            "   ‚úì Built 480 chunks.\n",
            "\n",
            "üìÑ [17/42] paper_017_efficientnet_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/efficientnet_2019.pdf\n",
            "   ‚úì Built 519 chunks.\n",
            "\n",
            "üìÑ [18/42] paper_018_elmo_2018_deep_contextualized\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/elmo_2018_deep_contextualized.pdf\n",
            "   ‚úì Built 86 chunks.\n",
            "\n",
            "üìÑ [19/42] paper_019_fcn_2014_fully_conv_networks\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/fcn_2014_fully_conv_networks.pdf\n",
            "   ‚úì Built 528 chunks.\n",
            "\n",
            "üìÑ [20/42] paper_020_glove_2014\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/glove_2014.pdf\n",
            "   ‚úì Built 516 chunks.\n",
            "\n",
            "üìÑ [21/42] paper_021_googlenet_2014_inception\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/googlenet_2014_inception.pdf\n",
            "   ‚úì Built 516 chunks.\n",
            "\n",
            "üìÑ [22/42] paper_022_gpt2_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/gpt2_2019.pdf\n",
            "   ‚úì Built 463 chunks.\n",
            "\n",
            "üìÑ [23/42] paper_023_gpt3_2020\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/gpt3_2020.pdf\n",
            "   ‚úì Built 811 chunks.\n",
            "\n",
            "üìÑ [24/42] paper_024_llava_2023\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/llava_2023.pdf\n",
            "   ‚úì Built 141 chunks.\n",
            "\n",
            "üìÑ [25/42] paper_025_longformer_2020\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/longformer_2020.pdf\n",
            "   ‚úì Built 561 chunks.\n",
            "\n",
            "üìÑ [26/42] paper_026_mask_rcnn_2017\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/mask_rcnn_2017.pdf\n",
            "   ‚úì Built 548 chunks.\n",
            "\n",
            "üìÑ [27/42] paper_027_mobilenet_v1_2017\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/mobilenet_v1_2017.pdf\n",
            "   ‚úì Built 506 chunks.\n",
            "\n",
            "üìÑ [28/42] paper_028_mobilenet_v2_2018\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/mobilenet_v2_2018.pdf\n",
            "   ‚úì Built 533 chunks.\n",
            "\n",
            "üìÑ [29/42] paper_029_moco_v1_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/moco_v1_2019.pdf\n",
            "   ‚úì Built 542 chunks.\n",
            "\n",
            "üìÑ [30/42] paper_030_moco_v2_2020\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/moco_v2_2020.pdf\n",
            "   ‚úì Built 469 chunks.\n",
            "\n",
            "üìÑ [31/42] paper_031_resnet_2015_deep_residual\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/resnet_2015_deep_residual.pdf\n",
            "   ‚úì Built 543 chunks.\n",
            "\n",
            "üìÑ [32/42] paper_032_roberta_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/roberta_2019.pdf\n",
            "   ‚úì Built 75 chunks.\n",
            "\n",
            "üìÑ [33/42] paper_033_seq2seq_2014\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/seq2seq_2014.pdf\n",
            "   ‚úì Built 504 chunks.\n",
            "\n",
            "üìÑ [34/42] paper_034_simclr_2020\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/simclr_2020.pdf\n",
            "   ‚úì Built 573 chunks.\n",
            "\n",
            "üìÑ [35/42] paper_035_t5_2019_exploring_tl\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/t5_2019_exploring_tl.pdf\n",
            "   ‚úì Built 769 chunks.\n",
            "\n",
            "üìÑ [36/42] paper_036_unet_2015\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/unet_2015.pdf\n",
            "   ‚úì Built 30 chunks.\n",
            "\n",
            "üìÑ [37/42] paper_037_vgg_2014_very_deep\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/vgg_2014_very_deep.pdf\n",
            "   ‚úì Built 536 chunks.\n",
            "\n",
            "üìÑ [38/42] paper_038_vilbert_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/vilbert_2019.pdf\n",
            "   ‚úì Built 522 chunks.\n",
            "\n",
            "üìÑ [39/42] paper_039_word2vec_2013\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/word2vec_2013.pdf\n",
            "   ‚úì Built 510 chunks.\n",
            "\n",
            "üìÑ [40/42] paper_040_xlnet_2019\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/xlnet_2019.pdf\n",
            "   ‚úì Built 87 chunks.\n",
            "\n",
            "üìÑ [41/42] paper_041_yolov1_2015\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/yolov1_2015.pdf\n",
            "   ‚úì Built 516 chunks.\n",
            "\n",
            "üìÑ [42/42] paper_042_yolov3_2018\n",
            "   File: /content/drive/MyDrive/UNISEARCH_MASTER/raw/papers/yolov3_2018.pdf\n",
            "   ‚úì Built 35 chunks.\n",
            "\n",
            "üßæ Wrote paper passages: /content/drive/MyDrive/UNISEARCH_MASTER/processed/manifests/paper_passages.jsonl\n",
            "   Papers processed          : 42\n",
            "   Papers skipped            : 0\n",
            "   Total paper chunks written: 17497\n",
            "\n",
            "‚úÖ CELL 9 complete ‚Äî paper passages ready for embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 10 ‚Äî Build ONLY BGE Text Embeddings\n",
        "# (lecture_passages + paper_passages)\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ------------------------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------------------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "MANIFEST = ROOT / \"processed/manifests\"\n",
        "EMB_ROOT = ROOT / \"processed/embeddings\"\n",
        "\n",
        "EMB_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LECTURE_PASSAGES = MANIFEST / \"lecture_passages.jsonl\"\n",
        "PAPER_PASSAGES = MANIFEST / \"paper_passages.jsonl\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# LOAD MODEL (BGE-large-en-v1.5)\n",
        "# ------------------------------------------------\n",
        "print(\"üî∑ Loading BGE-large-en-v1.5 (text encoder)...\")\n",
        "\n",
        "bge = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
        "bge.max_seq_length = 512\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "bge.to(device)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# HELPERS\n",
        "# ------------------------------------------------\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "def embed_texts(texts, model):\n",
        "    return model.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------\n",
        "# LOAD PASSAGES\n",
        "# ------------------------------------------------\n",
        "print(\"üìö Loading lecture passages...\")\n",
        "lecture_data = load_jsonl(LECTURE_PASSAGES)\n",
        "\n",
        "print(\"üìö Loading paper passages...\")\n",
        "paper_data = load_jsonl(PAPER_PASSAGES)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# BUILD BGE TEXT EMBEDDINGS\n",
        "# ------------------------------------------------\n",
        "print(\"\\nüî∑ Building BGE text embeddings...\")\n",
        "\n",
        "all_texts = []\n",
        "all_meta = []\n",
        "\n",
        "for row in tqdm(lecture_data, desc=\"Lecture passages\"):\n",
        "    all_texts.append(row[\"text\"])\n",
        "    all_meta.append({\"type\": \"lecture\", **row})\n",
        "\n",
        "for row in tqdm(paper_data, desc=\"Paper passages\"):\n",
        "    all_texts.append(row[\"text\"])\n",
        "    all_meta.append({\"type\": \"paper\", **row})\n",
        "\n",
        "text_embeddings = embed_texts(all_texts, bge)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# SAVE\n",
        "# ------------------------------------------------\n",
        "np.save(EMB_ROOT / \"text_embeddings.npy\", text_embeddings)\n",
        "\n",
        "with open(EMB_ROOT / \"text_meta.jsonl\", \"w\") as f:\n",
        "    for m in all_meta:\n",
        "        f.write(json.dumps(m) + \"\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Saved BGE text embeddings + metadata.\")\n",
        "print(\"‚úÖ CELL 10 COMPLETE ‚Äî text embeddings ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "24ff69ee190e4d748f4d566e9a2306a9",
            "42d9381222a54150bc5d6ca11e4c3264",
            "218385697efe4de7b91e08b881fba55b",
            "9b1180f58230494fb1c5551448a83c56",
            "8619d9bbed104cb1bd95d4dc94a45892",
            "602a16ae013a4ac39c1355dc0125c1c8",
            "8fe7d4e5ceb348f884bfebd19bd2f578",
            "1ee62543617c4096a1b93c0131e6cd2e",
            "d634e49d53f84c30b1bb7f0226e95562",
            "2da38cbf537b44d69a02e06de74c569d",
            "b5cbe87c968741b1ab55e3efd892c6db"
          ]
        },
        "id": "kzL13k69pZ1I",
        "outputId": "6c4c4de4-0db7-4f40-b47c-e7bc70452867"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî∑ Loading BGE-large-en-v1.5 (text encoder)...\n",
            "üìö Loading lecture passages...\n",
            "üìö Loading paper passages...\n",
            "\n",
            "üî∑ Building BGE text embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Lecture passages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20624/20624 [00:00<00:00, 938947.18it/s]\n",
            "Paper passages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17497/17497 [00:00<00:00, 1019684.83it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1192 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24ff69ee190e4d748f4d566e9a2306a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Saved BGE text embeddings + metadata.\n",
            "‚úÖ CELL 10 COMPLETE ‚Äî text embeddings ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 11 ‚Äî Build SigLIP Image Embeddings (FIXED)\n",
        "# (Keyframes ‚Üí Dense Vectors)\n",
        "#\n",
        "# - Reads keyframes_manifest.jsonl\n",
        "# - Uses SigLIP to embed each keyframe image\n",
        "# - Saves:\n",
        "#     /processed/embeddings/image_embeddings.npy\n",
        "#     /processed/embeddings/image_meta.jsonl\n",
        "# - Resumable: if files already exist, only new frames are added\n",
        "# - Robust: handles different path field names in manifest\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "\n",
        "# -------------------------------\n",
        "# CONFIG & PATHS\n",
        "# -------------------------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "MANIFEST_ROOT = ROOT / \"processed\" / \"manifests\"\n",
        "EMB_ROOT = ROOT / \"processed\" / \"embeddings\"\n",
        "\n",
        "EMB_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "KEYFRAMES_MANIFEST = MANIFEST_ROOT / \"keyframes_manifest.jsonl\"\n",
        "\n",
        "IMG_EMB_PATH = EMB_ROOT / \"image_embeddings.npy\"\n",
        "IMG_META_PATH = EMB_ROOT / \"image_meta.jsonl\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üíª Using device: {device}\")\n",
        "\n",
        "# -------------------------------\n",
        "# LOAD SIGLIP MODEL\n",
        "# -------------------------------\n",
        "print(\"üñºÔ∏è Loading SigLIP model (google/siglip-base-patch16-384)...\")\n",
        "processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-384\")\n",
        "model = AutoModel.from_pretrained(\"google/siglip-base-patch16-384\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# -------------------------------\n",
        "# UTILS\n",
        "# -------------------------------\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "def load_existing_image_embeddings():\n",
        "    \"\"\"\n",
        "    If image_embeddings.npy + image_meta.jsonl already exist,\n",
        "    load them so we can resume instead of recomputing everything.\n",
        "    \"\"\"\n",
        "    if not (IMG_EMB_PATH.exists() and IMG_META_PATH.exists()):\n",
        "        return None, []\n",
        "\n",
        "    print(f\"üîÅ Found existing image embeddings at: {IMG_EMB_PATH}\")\n",
        "    emb = np.load(IMG_EMB_PATH)\n",
        "\n",
        "    meta = []\n",
        "    with open(IMG_META_PATH, \"r\") as f:\n",
        "        for line in f:\n",
        "            meta.append(json.loads(line))\n",
        "\n",
        "    if emb.shape[0] != len(meta):\n",
        "        print(\"‚ö†Ô∏è WARNING: embedding rows != meta rows. \"\n",
        "              \"You may want to delete and recompute.\")\n",
        "    else:\n",
        "        print(f\"   Loaded {emb.shape[0]} existing image embeddings.\")\n",
        "\n",
        "    return emb, meta\n",
        "\n",
        "def get_image_rel_path(row):\n",
        "    \"\"\"\n",
        "    Try to determine the relative image path from a manifest row.\n",
        "    Handles multiple possible key names and falls back to a constructed path.\n",
        "    \"\"\"\n",
        "    # 1) Direct fields if they exist\n",
        "    for key in [\"file_path\", \"image_path\", \"frame_path\"]:\n",
        "        if key in row:\n",
        "            return row[key]\n",
        "\n",
        "    # 2) Fallback: construct from video_id + frame_id.\n",
        "    #    This matches the typical pattern:\n",
        "    #    processed/keyframes/{video_id}/{frame_id}.jpg\n",
        "    video_id = row.get(\"video_id\")\n",
        "    frame_id = row.get(\"frame_id\")\n",
        "    if video_id is None or frame_id is None:\n",
        "        raise KeyError(\n",
        "            \"Cannot determine image path: no file_path/image_path/frame_path \"\n",
        "            \"and missing video_id/frame_id in row.\"\n",
        "        )\n",
        "\n",
        "    # If your keyframes are .png instead of .jpg, change this extension.\n",
        "    return f\"processed/keyframes/{video_id}/{frame_id}.jpg\"\n",
        "\n",
        "def embed_image_batch(paths):\n",
        "    \"\"\"\n",
        "    Given a list of image paths, load them, run through SigLIP,\n",
        "    and return a numpy array of shape (batch_size, dim) plus\n",
        "    the list of successfully processed paths.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for p in paths:\n",
        "        try:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Failed to open image: {p} ({e})\")\n",
        "            images.append(None)\n",
        "\n",
        "    valid_indices = [i for i, img in enumerate(images) if img is not None]\n",
        "    if not valid_indices:\n",
        "        return None, []\n",
        "\n",
        "    valid_images = [images[i] for i in valid_indices]\n",
        "    valid_paths  = [paths[i] for i in valid_indices]\n",
        "\n",
        "    inputs = processor(images=valid_images, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        feats = model.get_image_features(**inputs)  # (B, D)\n",
        "    feats = feats.cpu().numpy()\n",
        "\n",
        "    # L2 normalize for cosine similarity\n",
        "    norms = np.linalg.norm(feats, axis=1, keepdims=True) + 1e-12\n",
        "    feats = feats / norms\n",
        "\n",
        "    return feats, valid_paths\n",
        "\n",
        "# -------------------------------\n",
        "# LOAD KEYFRAME METADATA\n",
        "# -------------------------------\n",
        "print(f\"üìÑ Loading keyframe manifest from: {KEYFRAMES_MANIFEST}\")\n",
        "keyframe_rows = load_jsonl(KEYFRAMES_MANIFEST)\n",
        "print(f\"   Total keyframes listed: {len(keyframe_rows)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# RESUME SUPPORT\n",
        "# -------------------------------\n",
        "existing_emb, existing_meta = load_existing_image_embeddings()\n",
        "processed_ids = set()\n",
        "\n",
        "if existing_meta:\n",
        "    processed_ids = {m[\"frame_id\"] for m in existing_meta if \"frame_id\" in m}\n",
        "    print(f\"   Already processed frame_ids: {len(processed_ids)}\")\n",
        "else:\n",
        "    print(\"   No existing image embeddings found ‚Äî starting fresh.\")\n",
        "\n",
        "# -------------------------------\n",
        "# BUILD NEW EMBEDDINGS\n",
        "# -------------------------------\n",
        "BATCH_SIZE = 64\n",
        "new_emb_list = []\n",
        "new_meta_list = []\n",
        "\n",
        "batch_paths = []\n",
        "batch_meta = []\n",
        "\n",
        "print(\"\\nüöÄ Embedding keyframes with SigLIP...\")\n",
        "\n",
        "for row in tqdm(keyframe_rows, desc=\"Keyframes\"):\n",
        "    frame_id = row.get(\"frame_id\")\n",
        "\n",
        "    # If we have frame_id and it's already processed, skip\n",
        "    if frame_id is not None and frame_id in processed_ids:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        rel_path = get_image_rel_path(row)\n",
        "    except KeyError as e:\n",
        "        print(f\"   ‚ö†Ô∏è Skipping row due to missing path info: {e}\")\n",
        "        continue\n",
        "\n",
        "    img_path = ROOT / rel_path\n",
        "    batch_paths.append(img_path)\n",
        "    batch_meta.append(row)\n",
        "\n",
        "    if len(batch_paths) >= BATCH_SIZE:\n",
        "        feats, valid_paths = embed_image_batch(batch_paths)\n",
        "\n",
        "        if feats is not None:\n",
        "            path_to_feat_idx = {str(p): i for i, p in enumerate(valid_paths)}\n",
        "            for meta_row, p in zip(batch_meta, batch_paths):\n",
        "                p_str = str(p)\n",
        "                if p_str in path_to_feat_idx:\n",
        "                    idx = path_to_feat_idx[p_str]\n",
        "                    new_emb_list.append(feats[idx])\n",
        "                    new_meta_list.append(meta_row)\n",
        "\n",
        "        batch_paths = []\n",
        "        batch_meta = []\n",
        "\n",
        "# Process any leftover images in the final batch\n",
        "if batch_paths:\n",
        "    feats, valid_paths = embed_image_batch(batch_paths)\n",
        "    if feats is not None:\n",
        "        path_to_feat_idx = {str(p): i for i, p in enumerate(valid_paths)}\n",
        "        for meta_row, p in zip(batch_meta, batch_paths):\n",
        "            p_str = str(p)\n",
        "            if p_str in path_to_feat_idx:\n",
        "                idx = path_to_feat_idx[p_str]\n",
        "                new_emb_list.append(feats[idx])\n",
        "                new_meta_list.append(meta_row)\n",
        "\n",
        "# -------------------------------\n",
        "# COMBINE WITH EXISTING (IF ANY)\n",
        "# -------------------------------\n",
        "if new_emb_list:\n",
        "    new_emb = np.vstack(new_emb_list)\n",
        "    print(f\"\\nüßÆ New embeddings computed this run: {new_emb.shape[0]}\")\n",
        "\n",
        "    if existing_emb is not None and existing_emb.shape[0] > 0:\n",
        "        combined_emb = np.vstack([existing_emb, new_emb])\n",
        "        combined_meta = existing_meta + new_meta_list\n",
        "    else:\n",
        "        combined_emb = new_emb\n",
        "        combined_meta = new_meta_list\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No new keyframes needed embedding (everything was already done).\")\n",
        "    combined_emb = existing_emb\n",
        "    combined_meta = existing_meta\n",
        "\n",
        "# -------------------------------\n",
        "# SAVE TO DISK\n",
        "# -------------------------------\n",
        "if combined_emb is not None and len(combined_meta) > 0:\n",
        "    np.save(IMG_EMB_PATH, combined_emb)\n",
        "\n",
        "    with open(IMG_META_PATH, \"w\") as f:\n",
        "        for m in combined_meta:\n",
        "            f.write(json.dumps(m) + \"\\n\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Saved image embeddings to: {IMG_EMB_PATH}\")\n",
        "    print(f\"‚úÖ Saved image metadata to : {IMG_META_PATH}\")\n",
        "    print(f\"   Total image embeddings: {combined_emb.shape[0]}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No image embeddings to save (something might be wrong).\")\n",
        "\n",
        "print(\"\\n‚úÖ CELL 11 COMPLETE ‚Äî SigLIP image embeddings ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zaW5OP-yaIl",
        "outputId": "e17ec489-798e-4fa6-bf67-fb6551b1133d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíª Using device: cuda\n",
            "üñºÔ∏è Loading SigLIP model (google/siglip-base-patch16-384)...\n",
            "üìÑ Loading keyframe manifest from: /content/drive/MyDrive/UNISEARCH_MASTER/processed/manifests/keyframes_manifest.jsonl\n",
            "   Total keyframes listed: 33212\n",
            "   No existing image embeddings found ‚Äî starting fresh.\n",
            "\n",
            "üöÄ Embedding keyframes with SigLIP...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Keyframes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33212/33212 [27:29<00:00, 20.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßÆ New embeddings computed this run: 33212\n",
            "\n",
            "‚úÖ Saved image embeddings to: /content/drive/MyDrive/UNISEARCH_MASTER/processed/embeddings/image_embeddings.npy\n",
            "‚úÖ Saved image metadata to : /content/drive/MyDrive/UNISEARCH_MASTER/processed/embeddings/image_meta.jsonl\n",
            "   Total image embeddings: 33212\n",
            "\n",
            "‚úÖ CELL 11 COMPLETE ‚Äî SigLIP image embeddings ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === CELL 12: Build FAISS IVF indices for text (BGE) and images (SigLIP) ========\n",
        "# This cell:\n",
        "#   1. Loads BGE text embeddings (lectures + papers) + their metadata.\n",
        "#   2. Loads SigLIP image embeddings (keyframes) + their metadata.\n",
        "#   3. Builds FAISS IndexIVFFlat (inverted file index, inner product) for both.\n",
        "#   4. Saves the indices to Drive for fast ANN (approximate nearest neighbor) search.\n",
        "#\n",
        "# Assumed files (from previous cells):\n",
        "#   - /processed/embeddings/text_embeddings_bge.npy\n",
        "#   - /processed/embeddings/text_meta_bge.jsonl\n",
        "#   - /processed/embeddings/image_embeddings.npy\n",
        "#   - /processed/embeddings/image_meta.jsonl\n",
        "#\n",
        "# NOTE:\n",
        "#   - We assume embeddings are already L2-normalized (so IP ‚âà cosine).\n",
        "#   - IVF is approximate but much faster than flat search, and more scalable.\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# --- Try to import FAISS, install if missing (Colab-friendly) -------------------\n",
        "try:\n",
        "    import faiss\n",
        "except ImportError:\n",
        "    print(\"üì¶ faiss not found, installing faiss-cpu...\")\n",
        "    %pip install -q faiss-cpu\n",
        "    import faiss\n",
        "\n",
        "# --- Paths ----------------------------------------------------------------------\n",
        "PROJECT_ROOT   = Path(\"/content/drive/MyDrive/UNISEARCH_MASTER\")\n",
        "PROCESSED_ROOT = PROJECT_ROOT / \"processed\"\n",
        "EMB_ROOT       = PROCESSED_ROOT / \"embeddings\"\n",
        "INDICES_ROOT   = PROCESSED_ROOT / \"indices\"\n",
        "\n",
        "INDICES_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Text (BGE) embeddings ‚Äî use the ones from your original Cell 10\n",
        "TEXT_EMB_PATH   = EMB_ROOT / \"text_embeddings.npy\"\n",
        "TEXT_META_PATH  = EMB_ROOT / \"text_meta.jsonl\"\n",
        "\n",
        "INDEX_TEXT_PATH = INDICES_ROOT / \"index_text_bge_ivf.faiss\"\n",
        "\n",
        "# Image (SigLIP) embeddings\n",
        "IMG_EMB_PATH     = EMB_ROOT / \"image_embeddings.npy\"\n",
        "IMG_META_PATH    = EMB_ROOT / \"image_meta.jsonl\"\n",
        "INDEX_IMAGE_PATH = INDICES_ROOT / \"index_image_siglip_ivf.faiss\"\n",
        "\n",
        "print(f\"üìÅ EMBEDDINGS ROOT: {EMB_ROOT}\")\n",
        "print(f\"üìÅ INDICES ROOT   : {INDICES_ROOT}\")\n",
        "\n",
        "# Helper: choose a reasonable nlist (number of IVF clusters)\n",
        "def choose_nlist(n_vectors: int) -> int:\n",
        "    # Heuristic: about sqrt(N), clipped into [64, 4096]\n",
        "    import math\n",
        "    nlist = int(math.sqrt(max(1, n_vectors)))\n",
        "    nlist = max(64, min(4096, nlist))\n",
        "    return nlist\n",
        "\n",
        "# === 1) Build / load TEXT IVF index (BGE) ======================================\n",
        "\n",
        "if not TEXT_EMB_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"‚ùå BGE text embeddings not found at: {TEXT_EMB_PATH}\\n\"\n",
        "        \"Make sure the BGE embeddings cell ran successfully.\"\n",
        "    )\n",
        "\n",
        "print(\"\\nüìö Loading BGE text embeddings...\")\n",
        "text_embs = np.load(TEXT_EMB_PATH)  # shape: [N_text, dim]\n",
        "num_text, dim_text = text_embs.shape\n",
        "print(f\"   ‚Üí Loaded {num_text} text embeddings with dim={dim_text}\")\n",
        "\n",
        "if not TEXT_META_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"‚ùå Text meta file not found at: {TEXT_META_PATH}\\n\"\n",
        "        \"It should have been written alongside the BGE embeddings.\"\n",
        "    )\n",
        "\n",
        "with TEXT_META_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    meta_lines = sum(1 for _ in f)\n",
        "if meta_lines != num_text:\n",
        "    print(\n",
        "        f\"‚ö†Ô∏è WARNING: text_meta_bge.jsonl line count ({meta_lines}) \"\n",
        "        f\"!= embeddings rows ({num_text}). \"\n",
        "        \"Index will still be built, but check your pipeline consistency.\"\n",
        "    )\n",
        "\n",
        "if INDEX_TEXT_PATH.exists():\n",
        "    print(f\"\\nüì¶ Existing BGE IVF text index found at: {INDEX_TEXT_PATH}\")\n",
        "    index_text = faiss.read_index(str(INDEX_TEXT_PATH))\n",
        "    if index_text.ntotal != num_text:\n",
        "        print(\n",
        "            f\"‚ö†Ô∏è Index contains {index_text.ntotal} vectors, \"\n",
        "            f\"but we have {num_text} text embeddings.\"\n",
        "        )\n",
        "        print(\"   If this is stale, delete the .faiss file and re-run this cell.\")\n",
        "    else:\n",
        "        print(\"   ‚úì Text IVF index loaded and matches embedding count.\")\n",
        "else:\n",
        "    print(\"\\nüßÆ Building new FAISS IVF index for BGE text embeddings...\")\n",
        "\n",
        "    # L2 normalize again just in case (cheap and safe)\n",
        "    text_embs = text_embs.astype(\"float32\")\n",
        "    norms = np.linalg.norm(text_embs, axis=1, keepdims=True) + 1e-10\n",
        "    text_embs = text_embs / norms\n",
        "\n",
        "    nlist_text = choose_nlist(num_text)\n",
        "    print(f\"   ‚Üí Using nlist (clusters) for text: {nlist_text}\")\n",
        "\n",
        "    # Quantizer for IVF: flat inner-product index\n",
        "    quantizer_text = faiss.IndexFlatIP(dim_text)\n",
        "    index_text = faiss.IndexIVFFlat(quantizer_text, dim_text, nlist_text, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "    # Train IVF on the text embeddings\n",
        "    print(\"   ‚Üí Training text IVF index...\")\n",
        "    index_text.train(text_embs)\n",
        "    print(\"   ‚úì Training complete.\")\n",
        "\n",
        "    # Add all text vectors\n",
        "    index_text.add(text_embs)\n",
        "    print(f\"   ‚Üí IVF text index built with {index_text.ntotal} vectors.\")\n",
        "\n",
        "    faiss.write_index(index_text, str(INDEX_TEXT_PATH))\n",
        "    print(f\"‚úÖ Saved text IVF index to: {INDEX_TEXT_PATH}\")\n",
        "\n",
        "# === 2) Build / load IMAGE IVF index (SigLIP) ==================================\n",
        "\n",
        "if not IMG_EMB_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"\\n‚ùå SigLIP image embeddings not found at: {IMG_EMB_PATH}\\n\"\n",
        "        \"Make sure the SigLIP keyframe embedding cell ran successfully.\"\n",
        "    )\n",
        "\n",
        "print(\"\\nüñºÔ∏è Loading SigLIP image embeddings...\")\n",
        "img_embs = np.load(IMG_EMB_PATH)  # shape: [N_img, dim]\n",
        "num_img, dim_img = img_embs.shape\n",
        "print(f\"   ‚Üí Loaded {num_img} image embeddings with dim={dim_img}\")\n",
        "\n",
        "if not IMG_META_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"‚ùå Image meta file not found at: {IMG_META_PATH}\\n\"\n",
        "        \"It should have been written in the SigLIP cell.\"\n",
        "    )\n",
        "\n",
        "with IMG_META_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    img_meta_lines = sum(1 for _ in f)\n",
        "if img_meta_lines != num_img:\n",
        "    print(\n",
        "        f\"‚ö†Ô∏è WARNING: image_meta.jsonl line count ({img_meta_lines}) \"\n",
        "        f\"!= embeddings rows ({num_img}). \"\n",
        "        \"Index will still be built, but check your pipeline consistency.\"\n",
        "    )\n",
        "\n",
        "if INDEX_IMAGE_PATH.exists():\n",
        "    print(f\"\\nüì¶ Existing SigLIP IVF image index found at: {INDEX_IMAGE_PATH}\")\n",
        "    index_img = faiss.read_index(str(INDEX_IMAGE_PATH))\n",
        "    if index_img.ntotal != num_img:\n",
        "        print(\n",
        "            f\"‚ö†Ô∏è Index contains {index_img.ntotal} vectors, \"\n",
        "            f\"but we have {num_img} image embeddings.\"\n",
        "        )\n",
        "        print(\"   If this is stale, delete the .faiss file and re-run this cell.\")\n",
        "    else:\n",
        "        print(\"   ‚úì Image IVF index loaded and matches embedding count.\")\n",
        "else:\n",
        "    print(\"\\nüßÆ Building new FAISS IVF index for SigLIP image embeddings...\")\n",
        "\n",
        "    img_embs = img_embs.astype(\"float32\")\n",
        "    norms = np.linalg.norm(img_embs, axis=1, keepdims=True) + 1e-10\n",
        "    img_embs = img_embs / norms\n",
        "\n",
        "    nlist_img = choose_nlist(num_img)\n",
        "    print(f\"   ‚Üí Using nlist (clusters) for images: {nlist_img}\")\n",
        "\n",
        "    quantizer_img = faiss.IndexFlatIP(dim_img)\n",
        "    index_img = faiss.IndexIVFFlat(quantizer_img, dim_img, nlist_img, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "    print(\"   ‚Üí Training image IVF index...\")\n",
        "    index_img.train(img_embs)\n",
        "    print(\"   ‚úì Training complete.\")\n",
        "\n",
        "    index_img.add(img_embs)\n",
        "    print(f\"   ‚Üí IVF image index built with {index_img.ntotal} vectors.\")\n",
        "\n",
        "    faiss.write_index(index_img, str(INDEX_IMAGE_PATH))\n",
        "    print(f\"‚úÖ Saved image IVF index to: {INDEX_IMAGE_PATH}\")\n",
        "\n",
        "print(\"\\n‚úÖ CELL 12 COMPLETE ‚Äî IVF FAISS indices for text (BGE) and images (SigLIP) are ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrGy1DhR4xxT",
        "outputId": "c4c6b351-afd1-49cb-88c7-bf33cfa58ce5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ EMBEDDINGS ROOT: /content/drive/MyDrive/UNISEARCH_MASTER/processed/embeddings\n",
            "üìÅ INDICES ROOT   : /content/drive/MyDrive/UNISEARCH_MASTER/processed/indices\n",
            "\n",
            "üìö Loading BGE text embeddings...\n",
            "   ‚Üí Loaded 38121 text embeddings with dim=1024\n",
            "\n",
            "üßÆ Building new FAISS IVF index for BGE text embeddings...\n",
            "   ‚Üí Using nlist (clusters) for text: 195\n",
            "   ‚Üí Training text IVF index...\n",
            "   ‚úì Training complete.\n",
            "   ‚Üí IVF text index built with 38121 vectors.\n",
            "‚úÖ Saved text IVF index to: /content/drive/MyDrive/UNISEARCH_MASTER/processed/indices/index_text_bge_ivf.faiss\n",
            "\n",
            "üñºÔ∏è Loading SigLIP image embeddings...\n",
            "   ‚Üí Loaded 33212 image embeddings with dim=768\n",
            "\n",
            "üßÆ Building new FAISS IVF index for SigLIP image embeddings...\n",
            "   ‚Üí Using nlist (clusters) for images: 182\n",
            "   ‚Üí Training image IVF index...\n",
            "   ‚úì Training complete.\n",
            "   ‚Üí IVF image index built with 33212 vectors.\n",
            "‚úÖ Saved image IVF index to: /content/drive/MyDrive/UNISEARCH_MASTER/processed/indices/index_image_siglip_ivf.faiss\n",
            "\n",
            "‚úÖ CELL 12 COMPLETE ‚Äî IVF FAISS indices for text (BGE) and images (SigLIP) are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNVKAxywE7A2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}