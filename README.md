# ğŸŒŒ UNISEARCH â€” Multimodal Academic Search Engine  
### ğŸ” *Search across lectures, transcripts, keyframes, and research papers in one unified system.*

![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)
![Colab](https://img.shields.io/badge/Run%20on-Colab-yellow?logo=googlecolab)
![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ğŸ” UNISEARCH â€” Cross-Modal Academic Search Engine

UNISEARCH is a production-style multimodal retrieval and RAG system for searching lecture videos, transcripts, images and research papers in a unified pipeline.

## ğŸš€ What It Does

Text â†’ Text / Image semantic search across lectures and papers

Image â†’ Text retrieval via aligned keyframes

Grounded QA with citations (lecture, timestamp, transcript snippet)

## ğŸ§  System Architecture
<p align="center"> <img width="5970" height="3570" alt="unisearch_v2_style1_glass" src="https://github.com/user-attachments/assets/8a80a29c-fe58-44f3-b424-4ecbaddad807" />
 </p>

## Pipeline

Multimodal ingestion (videos + PDFs)

Whisper transcription + keyframe extraction

Transcriptâ€“keyframe alignment

Hybrid retrieval (BM25 + dense)

FAISS vector search

RAG with citation-backed answers

## ğŸ› ï¸ Tech Stack

Embeddings: Fine-tuned BGE (text), SigLIP (images)

Indexing: FAISS

RAG: Gemma-4B (grounded generation only)

Backend: FastAPI

UI: Gradio

Evaluation: Recall, MRR, NDCG

## ğŸ“Š Evaluation Highlights
<p align="center"> <img width="1470" height="925" alt="Screenshot 2025-12-12 at 5 21 34â€¯PM" src="https://github.com/user-attachments/assets/61598681-d14e-4b77-8b6c-471ed5221953" /> </p>

Corpus: 38,121 academic passages

Test Set: 93 queries

Recall@300 improved 84% â†’ 96% using fine-tuned BGE

Significant gains in MRR and NDCG, especially on harder conceptual queries

## ğŸ”‘ Why It Matters

UNISEARCH demonstrates that retrieval qualityâ€”not just generationâ€”drives reliable QA.
Fine-tuning the retriever before generation led to more accurate, grounded, and trustworthy answers compared to LLM-only approaches.


## ğŸ› ï¸ Getting Started
1. Open **Phase 1** notebook in Colab â†’ run preprocessing, embedding, and indexing  
2. Open **Phase 2** notebook â†’ load FAISS indexes and manifests, run queries using Gradio UI and test multimodal retrieval with evaluation metrics  

---

## ğŸ“¬ Contact
If youâ€™d like to discuss multimodal retrieval, embeddings, or search engine design, feel free to reach out.

